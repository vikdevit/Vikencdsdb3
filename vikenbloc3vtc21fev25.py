# -*- coding: utf-8 -*-
"""VikenBloc3VTC21Fev25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggW8lBSDUuVfZanUyT59K0j_RZWSFJbD
"""

# Importation des bibliothèques
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
!pip install hdbscan
import hdbscan
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.model_selection import KFold
import folium
from folium.plugins import MarkerCluster
from IPython.display import IFrame

"""1 Exploration et pré-traitement des données"""

# Chargement et visualisation du dataset
pd.set_option('display.max_columns', None)
df = pd.read_csv("drive/MyDrive/Bloc3Viken/Bloc3-Ex2-uber.csv")
df.head(5)

# Afficher le nombre de lignes et colonnes du dataframe
df.shape

# Identifier les valeurs manquantes et explorer le type des données du dataframe
df.info()

# Créer un nouveau dataframe conservant les colonnes numériques, la colonne cab_type, la colonne datetime
colonnes_a_garder = df.select_dtypes(include=['number']).columns.tolist()
colonnes_a_garder.extend(['cab_type', 'datetime'])

df_new = df[colonnes_a_garder].copy()  # Créer le nouveau DataFrame

# Convertir la colonne datetime au format datetime et supprimer les colonnes id et timestamp
df_new = df_new.drop(columns=['id', 'timestamp'], errors='ignore')  # Supprimer sans erreur si colonne absente
df_new['datetime'] = pd.to_datetime(df_new['datetime'])  # Convertir datetime
df_new.info()  # Vérifier que datetime est bien en format datetime64
df_new.head()  # Afficher les premières lignes

# Remplir les valeurs manquantes de la colonne price par la moyenne de la colonne price

# Initialiser l'imputer avec la stratégie "mean"
imputer = SimpleImputer(strategy="mean")

# Appliquer l'imputation sur la colonne 'price' et conserver le résultat dans le DataFrame
df_new[['price']] = imputer.fit_transform(df_new[['price']])

# Vérifier si les valeurs manquantes ont été remplies
print(df_new['price'].isnull().sum())  # Doit afficher 0
df_new.info()

# Suppression des colonnes non pertinentes
colonnes_a_supprimer = [
    # Colonnes redondantes ou inutiles
    'apparentTemperature', 'temperatureHigh', 'temperatureLow',
    'apparentTemperatureHigh', 'apparentTemperatureLow',
    'visibility.1', 'dewPoint', 'pressure', 'ozone', 'moonPhase', 'windBearing',

    # Colonnes temporelles inutiles (suppression de la colonne month car seulement novembre et décembre)
    'windGustTime', 'temperatureHighTime', 'temperatureLowTime',
    'apparentTemperatureHighTime', 'apparentTemperatureLowTime',
    'sunriseTime', 'sunsetTime', 'uvIndexTime', 'month'

    # Autres colonnes non pertinentes
    'precipProbability', 'windGust', 'temperatureMin', 'temperatureMinTime',
    'temperatureMax', 'temperatureMaxTime', 'apparentTemperatureMin',
    'apparentTemperatureMinTime', 'apparentTemperatureMax',
    'apparentTemperatureMaxTime'
]

# Supprimer les colonnes
df_new = df_new.drop(columns=colonnes_a_supprimer, errors='ignore')

# Ajouter colonnes transformant l'heure en coordonnées circulaires
df_new["hour_sin"] = np.sin(2 * np.pi * df_new["hour"] / 24)
df_new["hour_cos"] = np.cos(2 * np.pi * df_new["hour"] / 24)

# Ajouter colonnes transformant l'heure en coordonnées circulaires
df_new['day_sin'] = np.sin(2 * np.pi * df_new['day'] / 7)
df_new['day_cos'] = np.cos(2 * np.pi * df_new['day'] / 7)

# Sélectionner uniquement les colonnes finales utiles
colonnes_finales = [
    'datetime', 'cab_type', 'hour', 'hour_sin', 'hour_cos', 'day', 'day_sin', 'day_cos', 'distance', 'latitude', 'longitude',
    'price', 'surge_multiplier', 'temperature', 'humidity', 'windSpeed',
    'precipIntensity', 'cloudCover', 'visibility'
]
df_clean = df_new[colonnes_finales].copy()

# Afficher un aperçu du DataFrame nettoyé
print(df_clean.info())
print(df_clean.head())

# Coordonnées de Boston (référence)
lat_boston = 42.3601
long_boston = -71.0589

# Fonction pour calculer la distance avec la formule Haversine (en kilomètres)
def haversine(lat1, lon1, lat2, lon2):
    R = 6371  # Rayon de la Terre en kilomètres
    phi1 = np.radians(lat1)
    phi2 = np.radians(lat2)
    delta_phi = np.radians(lat2 - lat1)
    delta_lambda = np.radians(lon2 - lon1)

    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
    return R * c  # Retourne la distance en kilomètres

# Fonction pour calculer l'angle (en radians)
def calculate_angle(lat1, lon1, lat2, lon2):
    # Calcul de l'angle entre les points
    return np.arctan2(lon2 - lon1, lat2 - lat1)

# Appliquer la fonction pour calculer la distance et l'angle à chaque ligne
df_clean['distance_pol'] = df.apply(lambda row: haversine(lat_boston, long_boston, row['latitude'], row['longitude']), axis=1)
df_clean['angle'] = df.apply(lambda row: calculate_angle(lat_boston, long_boston, row['latitude'], row['longitude']), axis=1)

# Afficher un échantillon pour vérifier
print(df_clean[['latitude', 'longitude', 'distance_pol', 'angle']].head())

# Afficher les statistiques pour les colonnes numériques
df_clean.describe()

# Regrouper le nombre de courses par jour et par heure
df_grouped = df_clean.groupby(['day', 'hour']).size().reset_index(name='count')

# Créer le graphique
plt.figure(figsize=(18, 10))
sns.barplot(data=df_grouped, x='hour', y='count', hue='day', palette="Set2", width=0.8)

# Ajouter des labels
plt.xlabel('Heure de la journée', fontsize=14)
plt.ylabel('Nombre de courses', fontsize=14)
plt.title('Nombre de courses par heure pour chaque jour de la semaine', fontsize=16)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Modifier la légende
plt.legend(title="Jour de la semaine", labels=['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche'], fontsize=12, title_fontsize=14)

# Afficher le graphique
plt.show()

# Vérifier la corrélation entre les colonnes numériques (utile pour PCA)

# Sélectionner uniquement les colonnes numériques
df_num = df_clean.select_dtypes(include=['number'])

# Calculer la matrice de corrélation
corr_matrix = df_num.corr()
print(corr_matrix)

# Afficher la matrice de corrélation sous forme de heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, vmin=-1, vmax=1)

# Ajouter un titre
plt.title("Matrice de corrélation des variables numériques", fontsize=14)

# Afficher le graphique
plt.show()

# Création des dataframe Uber et Lyft avant normalisation et PCA
# Liste des colonnes à conserver
"""columns_to_keep = [
    "hour_sin", "hour_cos",  # Heure (périodique)
    "day_sin", "day_cos",  # Jour de la semaine (périodique)
    "latitude", "longitude", "angle", "distance",  # Géographie
    "price", "surge_multiplier",  # Tarification
    "temperature", "humidity", "windSpeed", "precipIntensity", "visibility", "cloudCover"  # Météo
]"""

columns_to_keep = [
    "hour_sin", "hour_cos",  # Heure (périodique)
    "day_sin", "day_cos",  # Jour de la semaine (périodique)
    "latitude", "longitude", "distance",  # Géographie
    "price", "surge_multiplier",  # Tarification
    "temperature", "humidity", "windSpeed", "precipIntensity"  # Météo
]


# Filtrage des DataFrames pour Lyft et Uber
df_pca_lyft = df_clean[df_clean["cab_type"] == "Lyft"][columns_to_keep].copy()
df_pca_uber = df_clean[df_clean["cab_type"] == "Uber"][columns_to_keep].copy()

# Vérification des tailles des datasets
print("Nombre de courses Lyft :", df_pca_lyft.shape[0])
print("Nombre de courses Uber :", df_pca_uber.shape[0])

# Affichage des premières lignes
df_pca_lyft.head(), df_pca_uber.head()

"""2 - Normalisation des données avant PCA"""

# Normalisation des données pour Lyft
scaler_lyft = StandardScaler()
df_pca_lyft_scaled = scaler_lyft.fit_transform(df_pca_lyft)

# Normalisation des données pour Uber
scaler_uber = StandardScaler()
df_pca_uber_scaled = scaler_uber.fit_transform(df_pca_uber)

# Vérification de la forme des matrices normalisées
print("Shape Lyft (normalisé) :", df_pca_lyft_scaled.shape)
print("Shape Uber (normalisé) :", df_pca_uber_scaled.shape)

"""3 - Analyse en composantes principales"""

# PCA pour Lyft
pca_lyft = PCA(n_components=8, svd_solver='auto')
pca_lyft_components = pca_lyft.fit_transform(df_pca_lyft_scaled)

# PCA pour Uber
pca_uber = PCA(n_components=8, svd_solver='auto')
pca_uber_components = pca_uber.fit_transform(df_pca_uber_scaled)

# Création des DataFrames avec les composantes principales pour Lyft et Uber
df_pca_lyft_components = pd.DataFrame(pca_lyft_components, columns=[f'PC{i+1}' for i in range(8)])
df_pca_uber_components = pd.DataFrame(pca_uber_components, columns=[f'PC{i+1}' for i in range(8)])

# Affichage des premières lignes pour vérifier
print("Composantes principales Lyft :")
print(df_pca_lyft_components.head())

print("Composantes principales Uber :")
print(df_pca_uber_components.head())

# Vérification de la variance expliquée par chaque composante principale
print("\nVariance expliquée par chaque composante pour Lyft :")
print(pca_lyft.explained_variance_ratio_)

print("\nVariance expliquée par chaque composante pour Uber :")
print(pca_uber.explained_variance_ratio_)

# Récupérer la variance expliquée par composante et la variance cumulée
explained_variance_ratio_lyft = pca_lyft.explained_variance_ratio_
explained_variance_ratio_uber = pca_uber.explained_variance_ratio_

# Création de la figure
plt.figure(figsize=(10, 6))

# Courbe de variance cumulée pour Lyft
plt.plot(np.arange(1, len(explained_variance_ratio_lyft) + 1), np.cumsum(explained_variance_ratio_lyft),
         marker='o', linestyle='--', color='b', label='Variance cumulée Lyft')

# Barplot de variance par composante pour Lyft
plt.bar(np.arange(1, len(explained_variance_ratio_lyft) + 1), explained_variance_ratio_lyft,
        alpha=0.7, color='#AEC6CF', label='Variance par composante Lyft')

# Courbe de variance cumulée pour Uber
plt.plot(np.arange(1, len(explained_variance_ratio_uber) + 1), np.cumsum(explained_variance_ratio_uber),
         marker='o', linestyle='--', color='g', label='Variance cumulée Uber')

# Barplot de variance par composante pour Uber
plt.bar(np.arange(1, len(explained_variance_ratio_uber) + 1), explained_variance_ratio_uber,
        alpha=0.7, color='#98FB98', label='Variance par composante Uber')

# Axe X avec des valeurs de 1 en 1
plt.xticks(np.arange(1, len(explained_variance_ratio_lyft) + 1, 1))

# Ligne horizontale pour les 90% de variance expliquée
plt.axhline(y=0.9, color='r', linestyle='--', linewidth=1, label='90% de la variance')

# Labels et titre
plt.xlabel('Composantes principales', fontsize=12, fontweight='bold')
plt.ylabel('Variance expliquée', fontsize=12, fontweight='bold')
plt.title('Courbe des Variances en Fonction des Composantes Principales (Lyft et Uber)', fontsize=14, fontweight='bold')

# Activation du fond gris pour la grille
plt.gca().set_facecolor('#f4f4f4')  # Fond gris

# Ajout d'un quadrillage visible en pointillés gris clair
plt.grid(True, color='gray', linestyle='--', linewidth=0.5)

# Ajout de la légende
plt.legend(frameon=True, facecolor='white', edgecolor='black')

# Affichage du graphique
plt.show()

# Imprimer les composantes principales pour Uber
print("Composantes principales pour Uber:")
print(pca_uber.components_)

# Imprimer les composantes principales pour Lyft
print("Composantes principales pour Lyft:")
print(pca_lyft.components_)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Uber
loadings_uber = pd.DataFrame(pca_uber.components_, columns=columns_to_keep)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Lyft
loadings_lyft = pd.DataFrame(pca_lyft.components_, columns=columns_to_keep)

# Afficher les DataFrames des poids (loadings) pour Uber et Lyft
print("Loadings pour Uber:")
print(loadings_uber)

print("\nLoadings pour Lyft:")
print(loadings_lyft)

"""4 - Sélection du modèle

"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Fonction pour la méthode du coude
def elbow_method(pca_features, K_range=range(2, 10)):
    inertia = []

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(pca_features)
        inertia.append(kmeans.inertia_)

    # Tracer la courbe du coude
    plt.figure(figsize=(8,5))
    plt.plot(K_range, inertia, marker='o', linestyle='--')
    plt.xlabel('Nombre de clusters')
    plt.ylabel('Inertie (distortion)')
    plt.title('Méthode du coude pour choisir K')
    plt.grid()
    plt.show()

# Appliquer la méthode du coude pour Uber
print("Méthode du coude pour Uber :")
elbow_method(pca_uber_components)  # Appliquer sur les données transformées par PCA pour Uber

# Appliquer la méthode du coude pour Lyft
print("Méthode du coude pour Lyft :")
elbow_method(pca_lyft_components)  # Appliquer sur les données transformées par PCA pour Lyft

# Réduction à 2 dimensions pour visualisation
#pca_2D = PCA(n_components=2)
#reduced_matrix_uber = pca_2D.fit_transform(df_pca_uber_scaled)
#reduced_matrix_lyft = pca_2D.fit_transform(df_pca_lyft_scaled)

# Choisir le nombre optimal de cluster selon la méthode du coude et appliquer Kmeans

# Appliquer KMeans sur les données réduites à 2 dimensions pour Uber
kmeans_uber = KMeans(n_clusters=5, random_state=42, n_init=10)
labels_uber = kmeans_uber.fit_predict(df_pca_uber_components)

# Appliquer KMeans sur les données réduites à 2 dimensions pour Lyft
kmeans_lyft = KMeans(n_clusters=5, random_state=42, n_init=10)
labels_lyft = kmeans_lyft.fit_predict(df_pca_lyft_components)

# Ajouter les labels des clusters
df_pca_uber_components['Cluster'] = labels_uber # Ajouter la colonne "Cluster" avec les labels KMeans
df_pca_lyft_components['Cluster'] = labels_lyft # Ajouter la colonne "Cluster" avec les labels KMeans

# Ajouter les labels des clusters aux DataFrames Uber et Lyft
df_uber_clustered = df_pca_uber.copy()  # Créer une copie du DataFrame Uber
df_uber_clustered['Cluster'] = labels_uber  # Ajouter la colonne "Cluster" avec les labels KMeans

df_lyft_clustered = df_pca_lyft.copy()  # Créer une copie du DataFrame Lyft
df_lyft_clustered['Cluster'] = labels_lyft  # Ajouter la colonne "Cluster" avec les labels KMeans

# Afficher les résultats
print("Clusters Uber:")
print(df_uber_clustered.head())

print("\nClusters Lyft:")
print(df_lyft_clustered.head())

# Réduction en 2D pour visualisation
pca_2D = PCA(n_components=2)
reduced_matrix_uber = pca_2D.fit_transform(df_pca_uber_components.drop(columns=['Cluster']))
reduced_matrix_lyft = pca_2D.fit_transform(df_pca_lyft_components.drop(columns=['Cluster']))

# Ajouter les labels de clusters pour visualisation
df_pca_uber_2D = pd.DataFrame(reduced_matrix_uber, columns=['PC1', 'PC2'])
df_pca_uber_2D['Cluster'] = labels_uber

df_pca_lyft_2D = pd.DataFrame(reduced_matrix_lyft, columns=['PC1', 'PC2'])
df_pca_lyft_2D['Cluster'] = labels_lyft

# Visualisation des clusters Uber et clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_uber_2D, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_uber_8D = kmeans_uber.cluster_centers_
centroids_uber_2D = pca_2D.transform(centroids_uber_8D)  # Projeter en 2D

# Affichage des centroïdes
plt.scatter(centroids_uber_2D[:, 0], centroids_uber_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (5 clusters) après PCA (8D) - Projection sur PC1 & PC2 (Uber)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes projetés en 2D (Uber) :")
print(centroids_uber_2D)

# Ajouter les labels de clusters dans le DataFrame Uber (df_pca_uber)
df_pca_uber['Cluster'] = labels_uber

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_uber = df_pca_uber.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Uber :")
print(cluster_summary_uber)

"""**Interprétation possible des clusters Uber**

-**Cluster 0** : Courses en soirée durant jours proches du week-end, dans le centre-ville (peut-être en sortie du travail) avec un prix moyen, sans surcharge de tarification et conditions météo modérées

-**Cluster 1** : Courses en matinée et en semaine en centre ville de Boston avec des prix plus élevés, souvent avec une forte humidité, un vent plus fort et des précipitations

-**Cluster 2** : Trajets de fin de journée en fin de semaine plus au sud-est du centre ville de Boston avec des prix moyens et une météo stable plus froide, moins venteuse et avec faibles précipitations

-**Cluster 3** : Trajets tôt le matin en début de semaine, dans des zones moins centrales (peut-être dans les banlieues), avec des températures modérées et un peu de pluie

-**Cluster 4** : Trajets du soir en milieu de semaine, plus au nord ou à l'ouest de Boston, en hiver avec des prix plus bas

-Les **clusters 0, 1 et 2** sont associés à des trajets dans des zones urbaines (proches du centre de Boston), tandis que les **clusters 3 et 4** semblent plus éloignés du centre-ville avec des trajets plus froids, souvent en dehors des heures de pointe.

**Interprétation possible des distances en clusters Uber**

-Les distances moyennes sont globalement similaires et courtes (~2.1 à 2.18 miles), ce qui est typique des trajets urbains ou semi-urbains dans des villes comme Boston.

 -Ces distances similaires en clusters montrent que les trajets courts sont la norme pour Uber, ce qui peut refléter la nature des services Uber, souvent utilisés pour des trajets rapides dans des zones urbaines ou suburbaines.
"""

# Visualisation des clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_lyft_2D, x='PC1', y='PC2', hue='Cluster', palette='Set2', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_lyft_8D = kmeans_lyft.cluster_centers_
centroids_lyft_2D = pca_2D.transform(centroids_lyft_8D)  # Projeter en 2D

# Affichage des centroïdes
plt.scatter(centroids_lyft_2D[:, 0], centroids_lyft_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (5 clusters) après PCA (8D) - Projection sur PC1 & PC2 (Lyft)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes projetés en 2D (Lyft) :")
print(centroids_lyft_2D)

# Ajouter les labels de clusters dans le DataFrame Lyft (df_pca_lyft)
df_pca_lyft['Cluster'] = labels_lyft

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_lyft = df_pca_lyft.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Lyft :")
print(cluster_summary_lyft)

"""**Interprétation possible des clusters Lyft**

-**Cluster 0** : Ce cluster pourrait représenter des trajets réguliers hors heures de pointes tous les jours de la semaine dans une zone centrale de Boston, durant des périodes où la demande est relativement stable et non influencée par des facteurs comme une surcharge élevée ou des conditions météorologiques extrêmes

-**Cluster 1** : Ce cluster pourrait représenter des trajets périodiques domicile-travail en heures de pointe, où une demande forte génère des prix plus élevés, et où la demande de Lyft se concentre dans des zones avec moins de véhicules disponibles ou dans des régions suburbaines de Boston

-**Cluster 2** : Ce cluster pourrait correspondre à des trajets occassionnels ou spécifiques dans la journée centrés autour de Boston. Il semble être plus typique et pourrait correspondre à des déplacements dans un cadre urbain modéré

-**Cluster 3** : Ce cluster pourrait être caractérisé par des trajets moins fréquents dans des zones suburbaines ou périphériques (trajets quotidiens en zones résidentielles), mais avec une demande stable et régulière

-**Cluster 4** : Ce cluster pourrait être caractérisé par des trajets effectués dans des conditions météorologiques moins favorables lors d'un mauvais temps avec vent fort dans des zones éloignées de Boston

**Interprétation possible des distances en clusters Lyft**

-Les distances parcourues sont similaires à celles pour les clusters Uber.

-Les trajets les plus longs (autour de 2.15 km) sont associés à des périodes de demande plus élevée, tandis que les trajets plus courts (2.02 km) sont probablement plus réguliers, à des prix moins élevés et avec une demande plus stable.

5- Evaluation du modèle
"""

# Calcul du Silhouette Score pour Uber
sil_score_uber = silhouette_score(reduced_matrix_uber, kmeans_uber.labels_)
print(f'Silhouette Score pour Uber: {sil_score_uber}')

# Calcul du Silhouette Score pour Lyft
sil_score_lyft = silhouette_score(reduced_matrix_lyft, kmeans_lyft.labels_)
print(f'Silhouette Score pour Lyft: {sil_score_lyft}')

# Validation croisée avec Kfold pour évaluation de la stabilité des clusters

# Fonction pour calculer la stabilité des clusters
def kfold_validation(X, n_clusters, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    kmeans_inertia = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        kmeans.fit(X_train)
        kmeans_inertia.append(kmeans.inertia_)  # Inertie sur les données d'entraînement

    # Calculer la moyenne de l'inertie à travers les folds
    mean_inertia = np.mean(kmeans_inertia)
    return mean_inertia

# Validation croisée pour Uber
mean_inertia_uber = kfold_validation(reduced_matrix_uber, n_clusters=5)
print(f'Moyenne de l\'inertie pour Uber (KFold): {mean_inertia_uber}')

# Validation croisée pour Lyft
mean_inertia_lyft = kfold_validation(reduced_matrix_lyft, n_clusters=5)
print(f'Moyenne de l\'inertie pour Lyft (KFold): {mean_inertia_lyft}')

# Evaluation de la compacité des clusters

# Inertie pour Uber
inertia_uber = kmeans_uber.inertia_
print(f'Inertie pour Uber: {inertia_uber}')

# Inertie pour Lyft
inertia_lyft = kmeans_lyft.inertia_
print(f'Inertie pour Lyft: {inertia_lyft}')

"""**Analyse des métriques d'évaluation**

**Silhouette score**

-Les scores sont faibles (silhouette score ~0.15), ce qui indique que les clusters se chevauchent légèrement et que la séparation entre eux n’est pas optimale.

-Cela peut signifier que :
  Les données sont complexes et difficiles à regrouper de manière claire (les clusters sont mieux séparés en 8D qu'en 2D)
  Certains clusters ont une structure moins distincte
  Peut-être qu’un autre nombre de clusters (via l’indice de Davies-Bouldin ou silhouette moyenne optimisée) améliorerait la performance

**Inertie (cohésion interne des clusters)**

-L’inertie de Lyft est légèrement plus faible, ce qui signifie que les points sont un peu plus proches des centroïdes que pour Uber.

-L’inertie KFold (validation croisée) est bien plus basse que l’inertie totale, suggérant que le modèle se généralise mieux sur des sous-ensembles des données que sur l’ensemble complet

**Peut-on améliorer le modèle**

-Les scores de silhouette sont faibles, donc les clusters ne sont pas idéalement séparés

-L’inertie est élevée, indiquant que les points ne sont pas fortement regroupés autour de leurs centroïdes

-En l’état, le clustering est exploitable mais pourrait être amélioré pour mieux capturer la structure des trajets Uber et Lyft.

6- Amélioration possibles du clustering

-Fine-Tuning du nombre de clusters en essayant k=3 (un chevauchement de clusters peut révéler que k=5 est trop élevé)

-Appliquer d’autres techniques de prétraitement des données pour réduire le bruit notamment pour Uber dont les clusters apparaissent moins bien séparés comparé à ceux de Lyft

-Tester un autre nombre de clusters (K) avec la méthode du coude ou le Silhouette Score moyen pour voir si une autre valeur offre une meilleure segmentation

-Essayer d’autres algorithmes comme DBSCAN (si les clusters ne sont pas de forme sphérique) ou Gaussian Mixture Models (GMM) pour capturer des structures plus complexes
"""