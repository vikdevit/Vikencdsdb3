# -*- coding: utf-8 -*-
"""VikenBloc3VTC21Fev25_check13Mars25_06Avril25vikenvik9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18T8M5GZGh9GDoQMtKyomnQwJmMhsBc9d
"""

from google.colab import drive
drive.mount('/content/drive') # Monte Google Drive vikenvik9@gmail.com

# Importation des bibliothèques
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.model_selection import KFold

"""1 Exploration et pré-traitement des données"""

# Chargement et visualisation du dataset
pd.set_option('display.max_columns', None)
df = pd.read_csv("drive/MyDrive/Bloc3Viken/Bloc3-Ex2-uber.csv")
df.head(5)

# Afficher le nombre de lignes et colonnes du dataframe
df.shape

# Identifier les valeurs manquantes et explorer le type des données du dataframe
df.info()

# Créer un nouveau dataframe conservant les colonnes numériques, la colonne cab_type, la colonne datetime
colonnes_a_garder = df.select_dtypes(include=['number']).columns.tolist()
colonnes_a_garder.extend(['cab_type', 'datetime'])

df_new = df[colonnes_a_garder].copy()  # Créer le nouveau DataFrame

# Convertir la colonne datetime au format datetime et supprimer les colonnes id et timestamp
df_new = df_new.drop(columns=['id', 'timestamp'], errors='ignore')  # Supprimer sans erreur si colonne absente
df_new['datetime'] = pd.to_datetime(df_new['datetime'])  # Convertir datetime
df_new.info()  # Vérifier que datetime est bien en format datetime64
df_new.head()  # Afficher les premières lignes

# Remplir les valeurs manquantes de la colonne price par la moyenne de la colonne price

# Initialiser l'imputer avec la stratégie "mean"
imputer = SimpleImputer(strategy="mean")

# Appliquer l'imputation sur la colonne 'price' et conserver le résultat dans le DataFrame
df_new[['price']] = imputer.fit_transform(df_new[['price']])

# Vérifier si les valeurs manquantes ont été remplies
print(df_new['price'].isnull().sum())  # Doit afficher 0
df_new.info()

# Suppression des colonnes non pertinentes
colonnes_a_supprimer = [
    # Colonnes redondantes ou inutiles
    'apparentTemperature', 'temperatureHigh', 'temperatureLow',
    'apparentTemperatureHigh', 'apparentTemperatureLow',
    'visibility.1', 'dewPoint', 'pressure', 'ozone', 'moonPhase', 'windBearing',

    # Colonnes temporelles inutiles (suppression de la colonne month car seulement novembre et décembre)
    'windGustTime', 'temperatureHighTime', 'temperatureLowTime',
    'apparentTemperatureHighTime', 'apparentTemperatureLowTime',
    'sunriseTime', 'sunsetTime', 'uvIndexTime', 'month'

    # Autres colonnes non pertinentes
    'precipProbability', 'windGust', 'temperatureMin', 'temperatureMinTime',
    'temperatureMax', 'temperatureMaxTime', 'apparentTemperatureMin',
    'apparentTemperatureMinTime', 'apparentTemperatureMax',
    'apparentTemperatureMaxTime'
]

# Supprimer les colonnes
df_new = df_new.drop(columns=colonnes_a_supprimer, errors='ignore')

# Créer une colonne "day_of_week" pour avoir les jours de la semaine
df_new['day_of_week'] = df_new['datetime'].dt.dayofweek  # 0 = Lundi, 6 = Dimanche

# Sélectionner uniquement les colonnes finales utiles
colonnes_finales = [
    'datetime', 'cab_type', 'hour', 'day', 'day_of_week', 'distance', 'latitude', 'longitude',
    'price', 'surge_multiplier', 'temperature', 'humidity', 'windSpeed',
    'precipIntensity', 'cloudCover', 'visibility'
]
df_clean = df_new[colonnes_finales].copy()

# Afficher un aperçu du DataFrame nettoyé
print(df_clean.info())
print(df_clean.head())

# Ajouter une colonne donnant le nom du jour de la semaine pour plus de clarté
df_clean['day_of_week_name'] = df_clean['day_of_week'].map({
    0: 'Lundi', 1: 'Mardi', 2: 'Mercredi', 3: 'Jeudi',
    4: 'Vendredi', 5: 'Samedi', 6: 'Dimanche'
})

# Regrouper le nombre de courses par heure de la journée et par jour de la semaine
df_grouped = df_clean.groupby(['day_of_week_name', 'hour']).size().reset_index(name='count')

# Créer le graphique
plt.figure(figsize=(18, 10))

# Définir la palette Set2 (7 couleurs, une pour chaque jour)
palette = sns.color_palette("Set2", n_colors=7)

# Créer le barplot avec la palette définie manuellement
ax = sns.barplot(data=df_grouped, x='hour', y='count', hue='day_of_week_name', palette=palette, width=0.8)

# Ajouter des labels
plt.xlabel('Heure de la journée', fontsize=14)
plt.ylabel('Nombre de courses', fontsize=14)
plt.title('Nombre de courses par heure pour chaque jour de la semaine', fontsize=16)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Modifier la légende pour que la couleur de chaque jour soit correctement affichée
# Nous accédons à la légende, et nous ajustons ses handles (les couleurs des légendes)
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles=handles, labels=['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche'],
          title="Jour de la semaine", fontsize=12, title_fontsize=14, bbox_to_anchor=(1, 1), loc='upper left')

# Afficher le graphique
plt.show()

# Ajouter colonnes transformant l'heure en coordonnées circulaires
df_clean["hour_sin"] = np.sin(2 * np.pi * df_clean["hour"] / 24)
df_clean["hour_cos"] = np.cos(2 * np.pi * df_clean["hour"] / 24)

# Afficher les statistiques pour les colonnes numériques
df_clean.describe()

# Visulaiser la distribution des colonnes
columns_to_plot = [
    'hour', 'day', 'day_of_week', 'distance', 'latitude', 'longitude',
    'price', 'surge_multiplier', 'temperature', 'humidity', 'windSpeed',
    'precipIntensity', 'cloudCover', 'visibility', 'hour_sin', 'hour_cos'
]

# Définir la taille de la grille pour les subplots
n_cols = 4
n_rows = (len(columns_to_plot) + n_cols - 1) // n_cols

# Créer la figure
fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))
axes = axes.flatten()

# Tracer chaque histogramme
for i, col in enumerate(columns_to_plot):
    axes[i].hist(df_clean[col], bins=30, color='skyblue', edgecolor='black')
    axes[i].set_title(f'Distribution de {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Fréquence')

# Supprimer les axes vides si nécessaire
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# Vérifier la corrélation entre les colonnes numériques (utile pour PCA)

# Sélectionner uniquement les colonnes numériques
df_num = df_clean.select_dtypes(include=['number'])

# Calculer la matrice de corrélation
corr_matrix = df_num.corr()
print(corr_matrix)

# Afficher la matrice de corrélation sous forme de heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, vmin=-1, vmax=1)

# Ajouter un titre
plt.title("Matrice de corrélation des variables numériques", fontsize=14)

# Afficher le graphique
plt.show()

# Création des dataframe Uber et Lyft avant normalisation et PCA
# Liste des colonnes à conserver

columns_to_keep = [
    "hour", "day_of_week",
    "hour_sin", "hour_cos", # Heure (périodique)
    "distance", "latitude", "longitude",  # Géographie
    "price",  # Tarification
    "temperature", "humidity", "windSpeed"  # Météo
]


# Filtrage des DataFrames pour Lyft et Uber
df_pca_lyft = df_clean[df_clean["cab_type"] == "Lyft"][columns_to_keep].copy()
df_pca_uber = df_clean[df_clean["cab_type"] == "Uber"][columns_to_keep].copy()

# Vérification des tailles des datasets
print("Nombre de courses Lyft :", df_pca_lyft.shape[0])
print("Nombre de courses Uber :", df_pca_uber.shape[0])

# Affichage des premières lignes
df_pca_lyft.head(), df_pca_uber.head()

"""2 - Normalisation des données avant PCA"""

# Normalisation des données pour Lyft
scaler_lyft = StandardScaler()
df_pca_lyft_scaled = scaler_lyft.fit_transform(df_pca_lyft)

# Normalisation des données pour Uber
scaler_uber = StandardScaler()
df_pca_uber_scaled = scaler_uber.fit_transform(df_pca_uber)

# Vérification de la forme des matrices normalisées
print("Shape Lyft (normalisé) :", df_pca_lyft_scaled.shape)
print("Shape Uber (normalisé) :", df_pca_uber_scaled.shape)

"""3 - Analyse en composantes principales"""

# PCA pour Lyft
pca_lyft = PCA(n_components=6, svd_solver='auto')
pca_lyft_components = pca_lyft.fit_transform(df_pca_lyft_scaled)

# PCA pour Uber
pca_uber = PCA(n_components=6, svd_solver='auto')
pca_uber_components = pca_uber.fit_transform(df_pca_uber_scaled)

# Création des DataFrames avec les composantes principales pour Lyft et Uber
df_pca_lyft_components = pd.DataFrame(pca_lyft_components, columns=[f'PC{i+1}' for i in range(6)])
df_pca_uber_components = pd.DataFrame(pca_uber_components, columns=[f'PC{i+1}' for i in range(6)])

# Affichage des premières lignes pour vérifier
print("Composantes principales Lyft :")
print(df_pca_lyft_components.head())

print("Composantes principales Uber :")
print(df_pca_uber_components.head())

# Vérification de la variance expliquée par chaque composante principale
print("\nVariance expliquée par chaque composante pour Lyft :")
print(pca_lyft.explained_variance_ratio_)

print("\nVariance expliquée par chaque composante pour Uber :")
print(pca_uber.explained_variance_ratio_)

# Récupérer la variance expliquée par composante et la variance cumulée
explained_variance_ratio_lyft = pca_lyft.explained_variance_ratio_
explained_variance_ratio_uber = pca_uber.explained_variance_ratio_

# Création de la figure
plt.figure(figsize=(10, 6))

# Courbe de variance cumulée pour Lyft
plt.plot(np.arange(1, len(explained_variance_ratio_lyft) + 1), np.cumsum(explained_variance_ratio_lyft),
         marker='o', linestyle='--', color='b', label='Variance cumulée Lyft')

# Barplot de variance par composante pour Lyft
plt.bar(np.arange(1, len(explained_variance_ratio_lyft) + 1), explained_variance_ratio_lyft,
        alpha=0.7, color='#AEC6CF', label='Variance par composante Lyft')

# Courbe de variance cumulée pour Uber
plt.plot(np.arange(1, len(explained_variance_ratio_uber) + 1), np.cumsum(explained_variance_ratio_uber),
         marker='o', linestyle='--', color='g', label='Variance cumulée Uber')

# Barplot de variance par composante pour Uber
plt.bar(np.arange(1, len(explained_variance_ratio_uber) + 1), explained_variance_ratio_uber,
        alpha=0.7, color='#98FB98', label='Variance par composante Uber')

# Axe X avec des valeurs de 1 en 1
plt.xticks(np.arange(1, len(explained_variance_ratio_lyft) + 1, 1))

# Ligne horizontale pour les 90% de variance expliquée
plt.axhline(y=0.9, color='r', linestyle='--', linewidth=1, label='90% de la variance')

# Rallonger l'axe des Y
plt.ylim(0, 1)  # Ajuster la limite de l'axe Y de 0 à 1 pour visualiser la droite à 90% de la variance expliquée

# Labels et titre
plt.xlabel('Composantes principales', fontsize=12, fontweight='bold')
plt.ylabel('Variance expliquée', fontsize=12, fontweight='bold')
plt.title('Courbe des Variances en Fonction des Composantes Principales (Lyft et Uber)', fontsize=14, fontweight='bold')

# Activation du fond gris pour la grille
plt.gca().set_facecolor('#f4f4f4')  # Fond gris

# Ajout d'un quadrillage visible en pointillés gris clair
plt.grid(True, color='gray', linestyle='--', linewidth=0.5)

# Ajout de la légende
plt.legend(frameon=True, facecolor='white', edgecolor='black')

# Affichage du graphique
plt.show()

# Imprimer les composantes principales pour Uber
print("Composantes principales pour Uber:")
print(pca_uber.components_)

# Imprimer les composantes principales pour Lyft
print("Composantes principales pour Lyft:")
print(pca_lyft.components_)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Uber
loadings_uber = pd.DataFrame(pca_uber.components_, columns=columns_to_keep)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Lyft
loadings_lyft = pd.DataFrame(pca_lyft.components_, columns=columns_to_keep)

# Afficher les DataFrames des poids (loadings) pour Uber et Lyft
print("Loadings pour Uber:")
print(loadings_uber)

print("\nLoadings pour Lyft:")
print(loadings_lyft)

"""4 - Sélection du modèle

"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Appliquer un fond gris pour le style
plt.style.use('bmh')

# Fonction pour la méthode du coude
def elbow_method(pca_features, K_range=range(2, 10)):
    inertia = []

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(pca_features)
        inertia.append(kmeans.inertia_)

    # Tracer la courbe du coude
    plt.figure(figsize=(8,5))
    plt.plot(K_range, inertia, marker='o', linestyle='--')
    plt.xlabel('Nombre de clusters')
    plt.ylabel('Inertie (distortion)')
    plt.title('Méthode du coude pour choisir K')
    plt.grid()
    plt.show()

# Appliquer la méthode du coude pour Uber
print("Méthode du coude pour Uber :")
elbow_method(pca_uber_components)  # Appliquer sur les données transformées par PCA pour Uber

# Appliquer la méthode du coude pour Lyft
print("Méthode du coude pour Lyft :")
elbow_method(pca_lyft_components)  # Appliquer sur les données transformées par PCA pour Lyft

# Choisir le nombre optimal de cluster selon la méthode du coude et appliquer Kmeans

# Appliquer KMeans sur les données réduites à 2 dimensions pour Uber
kmeans_uber = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_uber = kmeans_uber.fit_predict(df_pca_uber_components)

# Appliquer KMeans sur les données réduites à 2 dimensions pour Lyft
kmeans_lyft = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_lyft = kmeans_lyft.fit_predict(df_pca_lyft_components)

# Ajouter les labels des clusters
df_pca_uber_components['Cluster'] = labels_uber # Ajouter la colonne "Cluster" avec les labels KMeans
df_pca_lyft_components['Cluster'] = labels_lyft # Ajouter la colonne "Cluster" avec les labels KMeans

# Ajouter les labels des clusters aux DataFrames Uber et Lyft
df_uber_clustered = df_pca_uber.copy()  # Créer une copie du DataFrame Uber
df_uber_clustered['Cluster'] = labels_uber  # Ajouter la colonne "Cluster" avec les labels KMeans

df_lyft_clustered = df_pca_lyft.copy()  # Créer une copie du DataFrame Lyft
df_lyft_clustered['Cluster'] = labels_lyft  # Ajouter la colonne "Cluster" avec les labels KMeans

# Afficher les résultats
print("Clusters Uber:")
print(df_uber_clustered.head())

print("\nClusters Lyft:")
print(df_lyft_clustered.head())

# Réduction en 2D pour visualisation
pca_2D = PCA(n_components=2)
reduced_matrix_uber = pca_2D.fit_transform(df_pca_uber_components.drop(columns=['Cluster']))
reduced_matrix_lyft = pca_2D.fit_transform(df_pca_lyft_components.drop(columns=['Cluster']))

# Ajouter les labels de clusters pour visualisation
df_pca_uber_2D = pd.DataFrame(reduced_matrix_uber, columns=['PC1', 'PC2'])
df_pca_uber_2D['Cluster'] = labels_uber

df_pca_lyft_2D = pd.DataFrame(reduced_matrix_lyft, columns=['PC1', 'PC2'])
df_pca_lyft_2D['Cluster'] = labels_lyft

# Visualisation des clusters Uber et clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_uber_2D, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_uber_6D = kmeans_uber.cluster_centers_
centroids_uber_2D = pca_2D.transform(centroids_uber_6D)  # Projeter en 2D

# Affichage des centroïdes
plt.scatter(centroids_uber_2D[:, 0], centroids_uber_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (4 clusters) après PCA (6D) - Projection sur PC1 & PC2 (Uber)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes projetés en 2D (Uber) :")
print(centroids_uber_2D)

# Ajouter les labels de clusters dans le DataFrame Uber (df_pca_uber)
df_pca_uber['Cluster'] = labels_uber

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_uber = df_pca_uber.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Uber :")
print(cluster_summary_uber)

"""**Interprétation possible des clusters Uber**

-**Cluster 0** : Des courses en soirée, en milieu de semaine, plutôt urbaines, avec une température fraîche, un prix un peu élevé et du vent. Cela pourrait correspondre à des retours à la maison après le travail.

-**Cluster 1** : Courses en fin de matinée en semaine, température froide, humide, prix un peu moins élevé. Probablement des courses utilitaires, rendez-vous ou déplacements professionnels.

-**Cluster 2** : Des courses en périphérie, le week-end ou début de semaine, avec vent fort, humidité élevée, prix bas et distance courte. Cela pourrait correspondre à des trajets locaux ou résidentiels hors du centre.

-**Cluster 3** : Courses de nuit en semaine, au centre-ville, avec une distance courte mais prix élevé. Cela peut indiquer un effet de tarification nocturne ou surcharge (surge). Peut-être des trajets de fin de soirée ou début de matinée, après des sorties.



"""

# Visualisation des clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_lyft_2D, x='PC1', y='PC2', hue='Cluster', palette='Set2', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_lyft_6D = kmeans_lyft.cluster_centers_
centroids_lyft_2D = pca_2D.transform(centroids_lyft_6D)  # Projeter en 2D

# Affichage des centroïdes
plt.scatter(centroids_lyft_2D[:, 0], centroids_lyft_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (4 clusters) après PCA (6D) - Projection sur PC1 & PC2 (Lyft)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes projetés en 2D (Lyft) :")
print(centroids_lyft_2D)

# Ajouter les labels de clusters dans le DataFrame Lyft (df_pca_lyft)
df_pca_lyft['Cluster'] = labels_lyft

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_lyft = df_pca_lyft.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Lyft :")
print(cluster_summary_lyft)

"""**Interprétation possible des clusters Lyft**

-**Cluster 0** : Courses en milieu de semaine, en matinée, avec forte humidité, peu de vent, prix modéré, dans le centre-ville. Cela peut refléter des trajets quotidiens réguliers (courses pro ou RDV).

-**Cluster 1** : Des courses de nuit en semaine, avec une humidité plus modérée, prix élevé, peut-être à cause d'une surcharge nocturne. Cela ressemble au cluster de nuit observé chez Uber aussi.

-**Cluster 2** : Courses en périphérie, le week-end ou en début de semaine, en matinée, distance et prix bas, mais conditions météo plus difficiles (vent et humidité). Cela évoque des trajets locaux ou résidents.

-**Cluster 3** : Courses de fin de journée, avec distance plus élevée, prix maximum, moins d'humidité, mais vent fort. Probablement des trajets de retour en soirée avec surcharge, typiques d’un usage pro ou domicile-travail.

**Comparaison avec les clusters de Uber**

-**Cluster 0** : Bien qu’ils aient un lieu et jour similaires, ce ne sont pas les mêmes moments de la journée. Uber capture la fin de journée, alors que Lyft regroupe des courses de matinée. Uber semble profiter de trajets après le travail, alors que Lyft vise des utilisateurs en matinée, peut-être plus réguliers.

-**Cluster 1** : Ces clusters n’ont pas les mêmes horaires. Lyft regroupe les trajets de nuit, tandis qu'Uber est plus sur une matinée calme. Ce cluster Lyft pourrait refléter des courses nocturnes de sorties, avec des prix plus élevés (surcharge ?).

-**CLuster 2** : Très similaires entre Uber et Lyft. Courses locales, matinales, en début de semaine (ou week-end), en périphérie. Cela semble refléter des trajets résidentiels réguliers, à faible coût et courte distance.

-**Cluster 3** : Uber capture ici les courses nocturnes, tandis que Lyft cible les retours du soir, avec un prix et une température plus élevés. Uber a ici des trajets “after hours”, Lyft est plus en phase avec des horaires de pointe.

5- Evaluation du modèle
"""

# Calcul du Silhouette Score pour Uber
sil_score_uber = silhouette_score(reduced_matrix_uber, kmeans_uber.labels_)
print(f'Silhouette Score pour Uber: {sil_score_uber}')

# Calcul du Silhouette Score pour Lyft
sil_score_lyft = silhouette_score(reduced_matrix_lyft, kmeans_lyft.labels_)
print(f'Silhouette Score pour Lyft: {sil_score_lyft}')

# Calcul du score de silhouette global dans un espace à 8 dimensions (8 composantes principales)
silhouette_avg_uber = silhouette_score(df_pca_uber_components, labels_uber)
print(f"Score moyen de silhouette uber (8D) : {silhouette_avg_uber:.4f}")

silhouette_avg_lyft = silhouette_score(df_pca_lyft_components, labels_lyft)
print(f"Score moyen de silhouette lyft (8D) : {silhouette_avg_lyft:.4f}")

# Validation croisée avec Kfold pour évaluation de la stabilité des clusters

# Fonction pour calculer la stabilité des clusters
def kfold_validation(X, n_clusters, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    kmeans_inertia = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        kmeans.fit(X_train)
        kmeans_inertia.append(kmeans.inertia_)  # Inertie sur les données d'entraînement

    # Calculer la moyenne de l'inertie à travers les folds
    mean_inertia = np.mean(kmeans_inertia)
    return mean_inertia

# Validation croisée pour Uber
mean_inertia_uber = kfold_validation(reduced_matrix_uber, n_clusters=4)
print(f'Moyenne de l\'inertie pour Uber (KFold): {mean_inertia_uber}')

# Validation croisée pour Lyft
mean_inertia_lyft = kfold_validation(reduced_matrix_lyft, n_clusters=3)
print(f'Moyenne de l\'inertie pour Lyft (KFold): {mean_inertia_lyft}')

# Evaluation de la compacité des clusters

# Inertie pour Uber
inertia_uber = kmeans_uber.inertia_
print(f'Inertie pour Uber: {inertia_uber}')

# Inertie pour Lyft
inertia_lyft = kmeans_lyft.inertia_
print(f'Inertie pour Lyft: {inertia_lyft}')

"""**Analyse des métriques d'évaluation**

**Silhouette score**

Interprétation des résultats :

Silhouette Score pour Uber : 0.262

Silhouette Score pour Lyft : 0.264

Ces scores sont relativement faibles, ce qui indique que les clusters pour Uber et Lyft ne sont pas particulièrement bien séparés. Cela suggère que le modèle de clustering pourrait être amélioré, car les points au sein de chaque cluster ne sont pas très éloignés de ceux des autres clusters. Cependant, les scores sont très similaires pour Uber et Lyft, ce qui montre que la séparation des clusters est globalement comparable entre les deux services.

Score moyen de silhouette (pour 6 dimensions) :

Uber : 0.2886

Lyft : 0.2953

Ces scores sont un peu plus élevés que les scores de silhouette pour les clusters d'origine, ce qui indique que, lorsqu'on prend en compte 6 dimensions (probablement une approche avec plus de variables), la séparation des clusters devient légèrement meilleure.

**Inertie**

Inertie pour Uber : 26,035.93

Inertie pour Lyft : 22,346.53

L'inertie est plus faible pour Lyft que pour Uber, ce qui suggère que, pour les mêmes paramètres de clustering, les points de Lyft sont globalement plus proches de leurs centres de clusters que ceux d'Uber. Cela pourrait signifier que le modèle de clustering est légèrement plus cohérent ou compact pour Lyft, même si les scores de silhouette ne sont pas très élevés.

Moyenne de l'inertie pour Uber (KFold) : 3,546.74

Moyenne de l'inertie pour Lyft (KFold) : 4,117.91

L'inertie moyenne sur les splits KFold est également plus faible pour Uber que pour Lyft, ce qui montre que les clusters formés à partir des données Uber sont plus stables et homogènes (ils ont des points qui sont plus proches de leurs centres à travers différents sous-ensembles de données). Cela pourrait aussi signifier que la méthode de clustering est mieux adaptée pour Uber en termes de variance et de stabilité dans différents ensembles de données.

Synthèse :

Silhouette Scores faibles (0.26 et 0.27) pour Uber et Lyft suggèrent que les clusters ne sont pas parfaitement bien séparés, mais les deux services sont relativement similaires à cet égard.

Inertie plus faible pour Lyft (22,346.53 contre 26,035.93) suggère que les clusters Lyft sont plus compacts et mieux regroupés, tandis qu'Uber présente une plus grande dispersion.

Inertie moyenne sur KFold plus faible pour Uber (3,546.74 contre 4,117.91) montre que, sur différents sous-ensembles de données, les clusters Uber sont plus stables et cohérents.

Globalement, on peut conclure que Lyft a des clusters un peu plus compacts, mais Uber montre une meilleure stabilité sur les différents sous-ensembles de données, même si les deux services ont des scores de silhouette similaires indiquant une séparation modérée des clusters. Pour améliorer les résultats de clustering, il pourrait être utile d'explorer différentes méthodes ou de réajuster les paramètres de clustering.

=> Décision de réajuster les paramètres du clustering
"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Liste des colonnes à conserver
columns_to_keep = [
    "hour", #"day_of_week",
    "hour_sin", "hour_cos",  # Heure (périodique)
    #"day_sin", "day_cos",  # Jour de la semaine (périodique)
    "distance", #"latitude", "longitude", "distance",  # Géographie
    "price", #"surge_multiplier",  # Tarification
    "temperature", "humidity" #"windSpeed" # "precipIntensity"  # Météo
]


df_lyft_opti = df_clean[df_clean["cab_type"] == "Lyft"][columns_to_keep].copy()
df_uber_opti = df_clean[df_clean["cab_type"] == "Uber"][columns_to_keep].copy()

# Normalisation des données pour Lyft
scaler_lyft = StandardScaler()
df_pca_lyft_scaled = scaler_lyft.fit_transform(df_lyft_opti)

# Normalisation des données pour Uber
scaler_uber = StandardScaler()
df_pca_uber_scaled = scaler_uber.fit_transform(df_uber_opti)

# Vérification de la forme des matrices normalisées
print("Shape Lyft (normalisé) :", df_pca_lyft_scaled.shape)
print("Shape Uber (normalisé) :", df_pca_uber_scaled.shape)

# PCA pour Lyft
pca_lyft = PCA(n_components=2, svd_solver='auto')
pca_lyft_components = pca_lyft.fit_transform(df_pca_lyft_scaled)

# PCA pour Uber
pca_uber = PCA(n_components=2, svd_solver='auto')
pca_uber_components = pca_uber.fit_transform(df_pca_uber_scaled)

# Création des DataFrames avec les composantes principales pour Lyft et Uber
df_pca_lyft_components = pd.DataFrame(pca_lyft_components, columns=[f'PC{i+1}' for i in range(2)])
df_pca_uber_components = pd.DataFrame(pca_uber_components, columns=[f'PC{i+1}' for i in range(2)])

# Affichage des premières lignes pour vérifier
print("Composantes principales Lyft :")
print(df_pca_lyft_components.head())

print("Composantes principales Uber :")
print(df_pca_uber_components.head())

# Vérification de la variance expliquée par chaque composante principale
print("\nVariance expliquée par chaque composante pour Lyft :")
print(pca_lyft.explained_variance_ratio_)

print("\nVariance expliquée par chaque composante pour Uber :")
print(pca_uber.explained_variance_ratio_)

# Récupérer la variance expliquée par composante et la variance cumulée
explained_variance_ratio_lyft = pca_lyft.explained_variance_ratio_
explained_variance_ratio_uber = pca_uber.explained_variance_ratio_

# Création de la figure
plt.figure(figsize=(10, 6))

# Courbe de variance cumulée pour Lyft
plt.plot(np.arange(1, len(explained_variance_ratio_lyft) + 1), np.cumsum(explained_variance_ratio_lyft),
         marker='o', linestyle='--', color='b', label='Variance cumulée Lyft')

# Barplot de variance par composante pour Lyft
plt.bar(np.arange(1, len(explained_variance_ratio_lyft) + 1), explained_variance_ratio_lyft,
        alpha=0.7, color='#AEC6CF', label='Variance par composante Lyft')

# Courbe de variance cumulée pour Uber
plt.plot(np.arange(1, len(explained_variance_ratio_uber) + 1), np.cumsum(explained_variance_ratio_uber),
         marker='o', linestyle='--', color='g', label='Variance cumulée Uber')

# Barplot de variance par composante pour Uber
plt.bar(np.arange(1, len(explained_variance_ratio_uber) + 1), explained_variance_ratio_uber,
        alpha=0.7, color='#98FB98', label='Variance par composante Uber')

# Axe X avec des valeurs de 1 en 1
plt.xticks(np.arange(1, len(explained_variance_ratio_lyft) + 1, 1))

# Ligne horizontale pour les 90% de variance expliquée
plt.axhline(y=0.9, color='r', linestyle='--', linewidth=1, label='90% de la variance')

# Rallonger l'axe des Y
plt.ylim(0, 1)  # Ajuster la limite de l'axe Y de 0 à 1 pour visualiser la droite à 90% de la variance expliquée

# Labels et titre
plt.xlabel('Composantes principales', fontsize=12, fontweight='bold')
plt.ylabel('Variance expliquée', fontsize=12, fontweight='bold')
plt.title('Courbe des Variances en Fonction des Composantes Principales (Lyft et Uber)', fontsize=14, fontweight='bold')

# Activation du fond gris pour la grille
plt.gca().set_facecolor('#f4f4f4')  # Fond gris

# Ajout d'un quadrillage visible en pointillés gris clair
plt.grid(True, color='gray', linestyle='--', linewidth=0.5)

# Ajout de la légende
plt.legend(frameon=True, facecolor='white', edgecolor='black')

# Affichage du graphique
plt.show()

# Imprimer les composantes principales pour Uber
print("Composantes principales pour Uber:")
print(pca_uber.components_)

# Imprimer les composantes principales pour Lyft
print("Composantes principales pour Lyft:")
print(pca_lyft.components_)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Uber
loadings_uber = pd.DataFrame(pca_uber.components_, columns=columns_to_keep)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Lyft
loadings_lyft = pd.DataFrame(pca_lyft.components_, columns=columns_to_keep)

# Afficher les DataFrames des poids (loadings) pour Uber et Lyft
print("Loadings pour Uber:")
print(loadings_uber)

print("\nLoadings pour Lyft:")
print(loadings_lyft)

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Fonction pour la méthode du coude
def elbow_method(pca_features, K_range=range(2, 10)):
    inertia = []

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(pca_features)
        inertia.append(kmeans.inertia_)

    # Tracer la courbe du coude
    plt.figure(figsize=(8,5))
    plt.plot(K_range, inertia, marker='o', linestyle='--')
    plt.xlabel('Nombre de clusters')
    plt.ylabel('Inertie (distortion)')
    plt.title('Méthode du coude pour choisir K')
    plt.grid()
    plt.show()

# Appliquer la méthode du coude pour Uber
print("Méthode du coude pour Uber :")
elbow_method(pca_uber_components)  # Appliquer sur les données transformées par PCA pour Uber

# Appliquer la méthode du coude pour Lyft
print("Méthode du coude pour Lyft :")
elbow_method(pca_lyft_components)  # Appliquer sur les données transformées par PCA pour Lyft

# Choisir le nombre optimal de cluster selon la méthode du coude et appliquer Kmeans

# Appliquer KMeans sur les données réduites à 2 dimensions pour Uber
kmeans_uber = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_uber = kmeans_uber.fit_predict(df_pca_uber_components)

# Appliquer KMeans sur les données réduites à 2 dimensions pour Lyft
kmeans_lyft = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_lyft = kmeans_lyft.fit_predict(df_pca_lyft_components)

# Ajouter les labels des clusters
df_pca_uber_components['Cluster'] = labels_uber # Ajouter la colonne "Cluster" avec les labels KMeans
df_pca_lyft_components['Cluster'] = labels_lyft # Ajouter la colonne "Cluster" avec les labels KMeans

# Ajouter les labels des clusters aux DataFrames Uber et Lyft
df_uber_clustered = df_uber_opti.copy()  # Créer une copie du DataFrame Uber
df_uber_clustered['Cluster'] = labels_uber  # Ajouter la colonne "Cluster" avec les labels KMeans

df_lyft_clustered = df_lyft_opti.copy()  # Créer une copie du DataFrame Lyft
df_lyft_clustered['Cluster'] = labels_lyft  # Ajouter la colonne "Cluster" avec les labels KMeans

# Afficher les résultats
print("Clusters Uber:")
print(df_uber_clustered.head())

print("\nClusters Lyft:")
print(df_lyft_clustered.head())

# Visualisation des clusters Uber et clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_uber_components, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_uber_2D = kmeans_uber.cluster_centers_
#centroids_uber_2D = pca_2D.transform(centroids_uber_4D)  # Projeter en 2D

# Affichage des centroïdes
plt.scatter(centroids_uber_2D[:, 0], centroids_uber_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (3 clusters) après PCA (2D) - Projection sur PC1 & PC2 (Uber)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes Uber :")
print(centroids_uber_2D)

# Ajouter les labels de clusters dans le DataFrame Uber (df_pca_uber)
df_uber_opti['Cluster'] = labels_uber

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_uber = df_uber_opti.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Uber :")
print(cluster_summary_uber)

"""**-Cluster 0 :**  Ce cluster semble correspondre à des trajets effectués en soirée, avec une température modérément froide, une distance relativement courte et un prix modéré. L'humidité élevée pourrait indiquer des conditions météorologiques plus humides.

**-Cluster 1 :** Ce cluster semble indiquer des trajets matinaux avec une température froide, mais une distance plus longue que dans le Cluster 0. L'humidité très élevée pourrait être liée à des conditions de matinée avec de la brume ou de la pluie. Le prix plus élevé pourrait suggérer des trajets dans des zones avec une demande plus importante.

**-Cluster 2 :** Ce cluster ressemble à des trajets en soirée, avec une température plus élevée que celle du Cluster 0. La distance est plus longue et le prix également plus élevé. L'humidité élevée indique des conditions similaires au Cluster 0, mais avec des trajets plus longs et potentiellement plus chers.

**-Cluster 3 :** Ce cluster représente des trajets matinaux dans des conditions très froides, avec une distance courte et un prix relativement bas. L'humidité est encore assez élevée, suggérant une matinée potentiellement humide ou brumeuse.
"""

# Visualisation des clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_lyft_components, x='PC1', y='PC2', hue='Cluster', palette='Set2', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_lyft_2D = kmeans_lyft.cluster_centers_
#centroids_lyft_2D = pca_2D.transform(centroids_lyft_4D)  # Projeter en 2D

# Affichage des centroïdes
plt.scatter(centroids_lyft_2D[:, 0], centroids_lyft_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (3 clusters) après PCA (2D) - Projection sur PC1 & PC2 (Lyft)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes Lyft :")
print(centroids_lyft_2D)

# Ajouter les labels de clusters dans le DataFrame Lyft (df_pca_lyft)
df_lyft_opti['Cluster'] = labels_lyft

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_lyft = df_lyft_opti.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Lyft :")
print(cluster_summary_lyft)

"""**-Cluster 0 :** Ce cluster correspond à des trajets matinaux dans des conditions froides et humides, avec des distances relativement courtes et un prix modéré. L'humidité très élevée suggère que les trajets peuvent se produire par temps de pluie ou de brume, ce qui pourrait expliquer des prix bas (peu de demande pendant ces conditions).

**-Cluster 1 :**  Ce cluster représente des trajets en soirée, avec une température légèrement plus élevée que celle du Cluster 0. Les distances restent courtes et les prix sont modérés. L'humidité est plus faible que dans le Cluster 0, ce qui pourrait indiquer des conditions météorologiques moins défavorables. Le prix bas pourrait être associé à une demande faible durant cette période.

**-Cluster 2 :** Ce cluster semble correspondre à des trajets matinaux dans des conditions de froid extrême. Bien que les trajets soient plus longs que dans les autres clusters, le prix est également plus élevé. L'humidité relativement élevée indique que les trajets peuvent se produire sous des conditions météorologiques de brume, de pluie ou de neige. Cela pourrait expliquer la distance plus longue et le prix plus élevé, en raison d'une plus grande demande ou de conditions de conduite plus difficiles.

**-Cluster 3 :** Ce cluster représente également des trajets en soirée, avec des températures fraîches. Les distances sont plus longues et les prix plus élevés par rapport aux autres clusters. L'humidité modérée indique des conditions météorologiques moins extrêmes, mais les trajets plus longs et les prix plus élevés suggèrent une demande plus élevée ou des trajets dans des zones plus éloignées.

**Comparaison des clusters Uber et Lyft**

Heure des trajets :

Uber tend à avoir des trajets plus variés entre le matin et le soir (6 AM et 6 PM), avec un pic notable le soir dans les clusters 0 et 2.

Lyft a une tendance plus marquée vers les trajets matinaux (autour de 6 AM), sauf pour le Cluster 3 (en soirée).

Température :

Uber a des températures assez froides, avec une gamme allant de 34.57°F à 44.80°F.

Lyft a des températures qui varient entre 35.44°F et 42.63°F, légèrement plus froides dans certains clusters, mais généralement plus chaudes que celles de Uber.

Distance et Prix :

Uber semble avoir une gamme de distances et de prix plus modérés, avec des trajets plus courts dans certains clusters et des prix plus faibles, en particulier dans le Cluster 0.

Lyft, en revanche, a des trajets globalement plus longs et des prix plus élevés, particulièrement dans les clusters 2 et 3.

Humidité :

Uber semble avoir une humidité plus élevée dans les trajets matinaux (Cluster 1 et Cluster 0).

Lyft a également des niveaux d'humidité assez élevés, mais globalement plus faibles que ceux d'Uber, sauf dans le Cluster 0 où elle atteint des niveaux élevés.

Conclusion :

Les clusters de Lyft et Uber partagent des caractéristiques similaires en termes de température et d'humidité, mais avec des différences notables concernant l'heure des trajets, les distances parcourues, et les prix. Lyft tend à avoir des trajets plus longs avec des prix plus élevés, tandis qu'Uber semble offrir des trajets plus courts et moins coûteux, particulièrement en soirée. Les conditions météorologiques influencent fortement la demande dans les deux services, mais l'humidité semble légèrement plus élevée dans les trajets Uber.
"""

# Calcul du score de silhouette global dans un espace à 8 dimensions (8 composantes principales)
silhouette_score_uber = silhouette_score(df_pca_uber_components, labels_uber)
print(f"Score de silhouette uber (2D) : {silhouette_score_uber:.4f}")

silhouette_score_lyft = silhouette_score(df_pca_lyft_components, labels_lyft)
print(f"Score de silhouette lyft (2D) : {silhouette_score_lyft:.4f}")

# Fonction modifiée pour utiliser l'indexation pandas
def kfold_validation(X, n_clusters, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    kmeans_inertia = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Utiliser .iloc pour l'indexation pandas
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        kmeans.fit(X_train)
        kmeans_inertia.append(kmeans.inertia_)  # Inertie sur les données d'entraînement

    # Calculer la moyenne de l'inertie à travers les folds
    mean_inertia = np.mean(kmeans_inertia)
    return mean_inertia

# Validation croisée pour Uber
mean_inertia_uber = kfold_validation(df_pca_uber_components, n_clusters=4)
print(f'Moyenne de l\'inertie pour Uber (KFold): {mean_inertia_uber}')

# Validation croisée pour Lyft
mean_inertia_lyft = kfold_validation(df_pca_lyft_components, n_clusters=3)
print(f'Moyenne de l\'inertie pour Lyft (KFold): {mean_inertia_lyft}')

# Evaluation de la compacité des clusters

# Inertie pour Uber
inertia_uber = kmeans_uber.inertia_
print(f'Inertie pour Uber: {inertia_uber}')

# Inertie pour Lyft
inertia_lyft = kmeans_lyft.inertia_
print(f'Inertie pour Lyft: {inertia_lyft}')

"""**Analyse des métriques d'évaluation**

**Silhouette score**

Uber (2D) : 0.6109

Lyft (2D) : 0.5983

Interprétation :

Uber (0.6109) : Un score de 0.6109 est relativement élevé, ce qui indique que les clusters pour Uber sont bien séparés et que la majorité des points de données sont correctement assignés à leurs clusters respectifs. Cela signifie que les clusters sont relativement compacts et distincts les uns des autres.

Lyft (0.5983) : Un score de 0.5983 est légèrement inférieur à celui d'Uber, mais reste également un score acceptable. Cela suggère que les clusters pour Lyft sont également bien séparés, mais peut-être légèrement moins distincts ou plus "flous" que ceux d'Uber. Cependant, cette différence est minime, donc la qualité du clustering pour Lyft est encore assez bonne.

En résumé, Uber a un léger avantage en termes de la clarté de ses clusters par rapport à Lyft, mais les deux services ont des clusters assez compacts et distincts.

**Inertie moyenne (KFold)**

Moyenne de l'inertie pour Uber (KFold) : 3238.72

Moyenne de l'inertie pour Lyft (KFold) : 5384.17

Interprétation :

Uber (3238.72) : L'inertie pour Uber est plus faible, ce qui signifie que les clusters sont plus compacts et mieux définis. Les points de données dans chaque cluster sont plus proches du centre, ce qui est un signe positif de la qualité du clustering.

Lyft (5384.17) : L'inertie pour Lyft est plus élevée, ce qui suggère que les clusters sont moins compacts et que les points de données sont un peu plus dispersés autour du centre du cluster. Cela peut indiquer une moins bonne séparation ou une plus grande variance au sein des clusters.

En résumé, Uber montre des clusters plus compacts, tandis que les clusters de Lyft sont légèrement moins denses et plus dispersés.

Interprétation :

***Inertie***

Uber (4049.43) :

L'inertie pour Uber est plus élevée, ce qui signifie que les points de données dans les clusters d'Uber sont plus éloignés de leurs centres respectifs. En d'autres termes, les clusters sont moins compacts, et les données sont plus dispersées dans chaque cluster.

Cela peut indiquer que le modèle de clustering pour Uber a du mal à regrouper les points de manière très homogène, ou que les clusters eux-mêmes sont plus larges et moins bien définis.

Lyft (3273.99) :

L'inertie pour Lyft est plus faible que celle d'Uber, ce qui signifie que les clusters de Lyft sont plus compacts et que les points de données sont plus proches du centre de leurs clusters respectifs.

Cela suggère que le modèle de clustering pour Lyft a généré des clusters plus homogènes et plus denses, avec une meilleure définition des groupes.

Comparaison entre Uber et Lyft :
Lyft a une inertie plus faible que Uber, ce qui indique des clusters plus compacts et plus bien définis.

Uber, en revanche, présente des clusters qui sont plus dispersés, avec des points de données plus éloignés du centre, ce qui entraîne une inertie plus élevée.

Les clusters pour Lyft semblent être plus bien séparés et cohérents, tandis que ceux pour Uber sont plus dispersés, ce qui peut indiquer que le modèle de clustering pour Lyft est mieux adapté ou que les données de Lyft sont naturellement plus homogènes par rapport à celles d'Uber.

En résumé :

Lyft montre une meilleure compacité des clusters (inertie plus faible), suggérant des regroupements plus cohérents et denses.

Uber a des clusters plus dispersés (inertie plus élevée), ce qui peut indiquer une moins bonne séparation ou des clusters plus larges et moins homogènes.

Cela signifie que, dans l'ensemble, Lyft a un modèle de clustering plus efficace et compact comparé à Uber, du moins en fonction de l'inertie.

**Actions de suite:**

-Appliquer d’autres techniques de prétraitement des données pour réduire le bruit.

-Essayer d’autres algorithmes comme DBSCAN (si les clusters ne sont pas de forme sphérique) ou Gaussian Mixture Models (GMM) pour capturer des structures plus complexes
"""