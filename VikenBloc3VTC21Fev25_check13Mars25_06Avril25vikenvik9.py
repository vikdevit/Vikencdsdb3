# -*- coding: utf-8 -*-
"""03_VikenBloc3VTC21Fev25_check13Mars25_06Avril25vikenvik9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HwKxJp2wGHgiUkkMmOaYAMjGgAIb5LIg
"""

from google.colab import drive
drive.mount('/content/drive') # Monte Google Drive vikenvik9@gmail.com

# Importation des bibliothèques
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.model_selection import KFold

"""1 Exploration et pré-traitement des données"""

# Chargement et visualisation du dataset
pd.set_option('display.max_columns', None)
df = pd.read_csv("drive/MyDrive/Bloc3Viken/Bloc3-Ex2-uber.csv")
df.head(5)

# Afficher le nombre de lignes et colonnes du dataframe
df.shape

# Identifier les valeurs manquantes et explorer le type des données du dataframe
df.info()

# Créer un nouveau dataframe conservant les colonnes numériques, la colonne cab_type, la colonne datetime
colonnes_a_garder = df.select_dtypes(include=['number']).columns.tolist()
colonnes_a_garder.extend(['cab_type', 'datetime'])

df_new = df[colonnes_a_garder].copy()  # Créer le nouveau DataFrame

# Convertir la colonne datetime au format datetime et supprimer les colonnes id et timestamp
df_new = df_new.drop(columns=['id', 'timestamp'], errors='ignore')  # Supprimer sans erreur si colonne absente
df_new['datetime'] = pd.to_datetime(df_new['datetime'])  # Convertir datetime
df_new.info()  # Vérifier que datetime est bien en format datetime64
df_new.head()  # Afficher les premières lignes

# Remplir les valeurs manquantes de la colonne price par la moyenne de la colonne price

# Initialiser l'imputer avec la stratégie "mean"
imputer = SimpleImputer(strategy="mean")

# Appliquer l'imputation sur la colonne 'price' et conserver le résultat dans le DataFrame
df_new[['price']] = imputer.fit_transform(df_new[['price']])

# Vérifier si les valeurs manquantes ont été remplies
print(df_new['price'].isnull().sum())  # Doit afficher 0
df_new.info()

# Suppression des colonnes non pertinentes
colonnes_a_supprimer = [
    # Colonnes redondantes ou inutiles
    'apparentTemperature', 'temperatureHigh', 'temperatureLow',
    'apparentTemperatureHigh', 'apparentTemperatureLow',
    'visibility.1', 'dewPoint', 'pressure', 'ozone', 'moonPhase', 'windBearing',

    # Colonnes temporelles inutiles (suppression de la colonne month car seulement novembre et décembre)
    'windGustTime', 'temperatureHighTime', 'temperatureLowTime',
    'apparentTemperatureHighTime', 'apparentTemperatureLowTime',
    'sunriseTime', 'sunsetTime', 'uvIndexTime', 'month'

    # Autres colonnes non pertinentes
    'precipProbability', 'windGust', 'temperatureMin', 'temperatureMinTime',
    'temperatureMax', 'temperatureMaxTime', 'apparentTemperatureMin',
    'apparentTemperatureMinTime', 'apparentTemperatureMax',
    'apparentTemperatureMaxTime'
]

# Supprimer les colonnes
df_new = df_new.drop(columns=colonnes_a_supprimer, errors='ignore')

# Créer une colonne "day_of_week" pour avoir les jours de la semaine
df_new['day_of_week'] = df_new['datetime'].dt.dayofweek  # 0 = Lundi, 6 = Dimanche

# Sélectionner uniquement les colonnes finales utiles
colonnes_finales = [
    'datetime', 'cab_type', 'hour', 'day', 'day_of_week', 'distance', 'latitude', 'longitude',
    'price', 'surge_multiplier', 'temperature', 'humidity', 'windSpeed',
    'precipIntensity', 'cloudCover', 'visibility'
]
df_clean = df_new[colonnes_finales].copy()

# Afficher un aperçu du DataFrame nettoyé
print(df_clean.info())
print(df_clean.head())

# Ajouter une colonne donnant le nom du jour de la semaine pour plus de clarté
df_clean['day_of_week_name'] = df_clean['day_of_week'].map({
    0: 'Lundi', 1: 'Mardi', 2: 'Mercredi', 3: 'Jeudi',
    4: 'Vendredi', 5: 'Samedi', 6: 'Dimanche'
})

# Regrouper le nombre de courses par heure de la journée et par jour de la semaine
df_grouped = df_clean.groupby(['day_of_week_name', 'hour']).size().reset_index(name='count')

# Créer le graphique
plt.figure(figsize=(18, 10))

# Définir la palette Set2 (7 couleurs, une pour chaque jour)
palette = sns.color_palette("Set2", n_colors=7)

# Créer le barplot avec la palette définie manuellement
ax = sns.barplot(data=df_grouped, x='hour', y='count', hue='day_of_week_name', palette=palette, width=0.8)

# Ajouter des labels
plt.xlabel('Heure de la journée', fontsize=14)
plt.ylabel('Nombre de courses', fontsize=14)
plt.title('Nombre de courses par heure pour chaque jour de la semaine', fontsize=16)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Modifier la légende pour que la couleur de chaque jour soit correctement affichée
# Nous accédons à la légende, et nous ajustons ses handles (les couleurs des légendes)
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles=handles, labels=['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche'],
          title="Jour de la semaine", fontsize=12, title_fontsize=14, bbox_to_anchor=(1, 1), loc='upper left')

# Afficher le graphique
plt.show()

# Ajouter colonnes transformant l'heure en coordonnées circulaires
df_clean["hour_sin"] = np.sin(2 * np.pi * df_clean["hour"] / 24)
df_clean["hour_cos"] = np.cos(2 * np.pi * df_clean["hour"] / 24)

# Afficher les statistiques pour les colonnes numériques
df_clean.describe()

# Visulaiser la distribution des colonnes
columns_to_plot = [
    'hour', 'day', 'day_of_week', 'distance', 'latitude', 'longitude',
    'price', 'surge_multiplier', 'temperature', 'humidity', 'windSpeed',
    'precipIntensity', 'cloudCover', 'visibility', 'hour_sin', 'hour_cos'
]

# Définir la taille de la grille pour les subplots
n_cols = 4
n_rows = (len(columns_to_plot) + n_cols - 1) // n_cols

# Créer la figure
fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 4))
axes = axes.flatten()

# Tracer chaque histogramme
for i, col in enumerate(columns_to_plot):
    axes[i].hist(df_clean[col], bins=30, color='skyblue', edgecolor='black')
    axes[i].set_title(f'Distribution de {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Fréquence')

# Supprimer les axes vides si nécessaire
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# Vérifier la corrélation entre les colonnes numériques (utile pour PCA)

# Sélectionner uniquement les colonnes numériques
df_num = df_clean.select_dtypes(include=['number'])

# Calculer la matrice de corrélation
corr_matrix = df_num.corr()
print(corr_matrix)

# Afficher la matrice de corrélation sous forme de heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, vmin=-1, vmax=1)

# Ajouter un titre
plt.title("Matrice de corrélation des variables numériques", fontsize=14)

# Afficher le graphique
plt.show()

# Création des dataframe Uber et Lyft avant normalisation et PCA
# Liste des colonnes à conserver

columns_to_keep = [
    "hour", "day_of_week",
    "hour_sin", "hour_cos", # Heure (périodique)
    "distance", "latitude", "longitude",  # Géographie
    "price",  # Tarification
    "temperature", "humidity", "windSpeed"  # Météo
]


# Filtrage des DataFrames pour Lyft et Uber
df_pca_lyft = df_clean[df_clean["cab_type"] == "Lyft"][columns_to_keep].copy()
df_pca_uber = df_clean[df_clean["cab_type"] == "Uber"][columns_to_keep].copy()

# Vérification des tailles des datasets
print("Nombre de courses Lyft :", df_pca_lyft.shape[0])
print("Nombre de courses Uber :", df_pca_uber.shape[0])

# Affichage des premières lignes
df_pca_lyft.head(), df_pca_uber.head()

"""2 - Normalisation des données avant PCA"""

# Normalisation des données pour Lyft
scaler_lyft = StandardScaler()
df_pca_lyft_scaled = scaler_lyft.fit_transform(df_pca_lyft)

# Normalisation des données pour Uber
scaler_uber = StandardScaler()
df_pca_uber_scaled = scaler_uber.fit_transform(df_pca_uber)

# Vérification de la forme des matrices normalisées
print("Shape Lyft (normalisé) :", df_pca_lyft_scaled.shape)
print("Shape Uber (normalisé) :", df_pca_uber_scaled.shape)

"""3 - Analyse en composantes principales"""

# PCA pour Lyft
pca_lyft = PCA(n_components=6, svd_solver='auto')
pca_lyft_components = pca_lyft.fit_transform(df_pca_lyft_scaled)

# PCA pour Uber
pca_uber = PCA(n_components=6, svd_solver='auto')
pca_uber_components = pca_uber.fit_transform(df_pca_uber_scaled)

# Création des DataFrames avec les composantes principales pour Lyft et Uber
df_pca_lyft_components = pd.DataFrame(pca_lyft_components, columns=[f'PC{i+1}' for i in range(6)])
df_pca_uber_components = pd.DataFrame(pca_uber_components, columns=[f'PC{i+1}' for i in range(6)])

# Affichage des premières lignes pour vérifier
print("Composantes principales Lyft :")
print(df_pca_lyft_components.head())

print("Composantes principales Uber :")
print(df_pca_uber_components.head())

# Vérification de la variance expliquée par chaque composante principale
print("\nVariance expliquée par chaque composante pour Lyft :")
print(pca_lyft.explained_variance_ratio_)

print("\nVariance expliquée par chaque composante pour Uber :")
print(pca_uber.explained_variance_ratio_)

# Récupérer la variance expliquée par composante et la variance cumulée
explained_variance_ratio_lyft = pca_lyft.explained_variance_ratio_
explained_variance_ratio_uber = pca_uber.explained_variance_ratio_

# Création de la figure
plt.figure(figsize=(10, 6))

# Courbe de variance cumulée pour Lyft
plt.plot(np.arange(1, len(explained_variance_ratio_lyft) + 1), np.cumsum(explained_variance_ratio_lyft),
         marker='o', linestyle='--', color='b', label='Variance cumulée Lyft')

# Barplot de variance par composante pour Lyft
plt.bar(np.arange(1, len(explained_variance_ratio_lyft) + 1), explained_variance_ratio_lyft,
        alpha=0.7, color='#AEC6CF', label='Variance par composante Lyft')

# Courbe de variance cumulée pour Uber
plt.plot(np.arange(1, len(explained_variance_ratio_uber) + 1), np.cumsum(explained_variance_ratio_uber),
         marker='o', linestyle='--', color='g', label='Variance cumulée Uber')

# Barplot de variance par composante pour Uber
plt.bar(np.arange(1, len(explained_variance_ratio_uber) + 1), explained_variance_ratio_uber,
        alpha=0.7, color='#98FB98', label='Variance par composante Uber')

# Axe X avec des valeurs de 1 en 1
plt.xticks(np.arange(1, len(explained_variance_ratio_lyft) + 1, 1))

# Ligne horizontale pour les 90% de variance expliquée
plt.axhline(y=0.9, color='r', linestyle='--', linewidth=1, label='90% de la variance')

# Rallonger l'axe des Y
plt.ylim(0, 1)  # Ajuster la limite de l'axe Y de 0 à 1 pour visualiser la droite à 90% de la variance expliquée

# Labels et titre
plt.xlabel('Composantes principales', fontsize=12, fontweight='bold')
plt.ylabel('Variance expliquée', fontsize=12, fontweight='bold')
plt.title('Courbe des Variances en Fonction des Composantes Principales (Lyft et Uber)', fontsize=14, fontweight='bold')

# Activation du fond gris pour la grille
plt.gca().set_facecolor('#f4f4f4')  # Fond gris

# Ajout d'un quadrillage visible en pointillés gris clair
plt.grid(True, color='gray', linestyle='--', linewidth=0.5)

# Ajout de la légende
plt.legend(frameon=True, facecolor='white', edgecolor='black')

# Affichage du graphique
plt.show()

# Imprimer les composantes principales pour Uber
print("Composantes principales pour Uber:")
print(pca_uber.components_)

# Imprimer les composantes principales pour Lyft
print("Composantes principales pour Lyft:")
print(pca_lyft.components_)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Uber
loadings_uber = pd.DataFrame(pca_uber.components_, columns=columns_to_keep)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Lyft
loadings_lyft = pd.DataFrame(pca_lyft.components_, columns=columns_to_keep)

# Afficher les DataFrames des poids (loadings) pour Uber et Lyft
print("Loadings pour Uber:")
print(loadings_uber)

print("\nLoadings pour Lyft:")
print(loadings_lyft)

"""4 - Sélection du modèle

"""

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Appliquer un fond gris pour le style
plt.style.use('bmh')

# Fonction pour la méthode du coude
def elbow_method(pca_features, K_range=range(2, 10)):
    inertia = []

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(pca_features)
        inertia.append(kmeans.inertia_)

    # Tracer la courbe du coude
    plt.figure(figsize=(8,5))
    plt.plot(K_range, inertia, marker='o', linestyle='--')
    plt.xlabel('Nombre de clusters')
    plt.ylabel('Inertie (distortion)')
    plt.title('Méthode du coude pour choisir K')
    plt.grid()
    plt.show()

# Appliquer la méthode du coude pour Uber
print("Méthode du coude pour Uber :")
elbow_method(pca_uber_components)  # Appliquer sur les données transformées par PCA pour Uber

# Appliquer la méthode du coude pour Lyft
print("Méthode du coude pour Lyft :")
elbow_method(pca_lyft_components)  # Appliquer sur les données transformées par PCA pour Lyft

# Choisir le nombre optimal de cluster selon la méthode du coude et appliquer Kmeans

# Appliquer KMeans sur les données réduites à 6 dimensions pour Uber
kmeans_uber = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_uber = kmeans_uber.fit_predict(df_pca_uber_components)

# Appliquer KMeans sur les données réduites à 6 dimensions pour Lyft
kmeans_lyft = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_lyft = kmeans_lyft.fit_predict(df_pca_lyft_components)

# Ajouter les labels des clusters
df_pca_uber_components['Cluster'] = labels_uber # Ajouter la colonne "Cluster" avec les labels KMeans
df_pca_lyft_components['Cluster'] = labels_lyft # Ajouter la colonne "Cluster" avec les labels KMeans

# Ajouter les labels des clusters aux DataFrames Uber et Lyft
df_uber_clustered = df_pca_uber.copy()  # Créer une copie du DataFrame Uber
df_uber_clustered['Cluster'] = labels_uber  # Ajouter la colonne "Cluster" avec les labels KMeans

df_lyft_clustered = df_pca_lyft.copy()  # Créer une copie du DataFrame Lyft
df_lyft_clustered['Cluster'] = labels_lyft  # Ajouter la colonne "Cluster" avec les labels KMeans

# Afficher les résultats
print("Clusters Uber:")
print(df_uber_clustered.head())

print("\nClusters Lyft:")
print(df_lyft_clustered.head())

# Projeter les données de l'espace 6D vers un espace 2D avec les deux premières composantes principales pour visualisation
df_pca_uber_2D = df_pca_uber_components[['PC1', 'PC2']].copy()
df_pca_uber_2D['Cluster'] = labels_uber

df_pca_lyft_2D = df_pca_lyft_components[['PC1', 'PC2']].copy()
df_pca_lyft_2D['Cluster'] = labels_lyft

# Visualisation des clusters Uber dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_uber_2D, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=50, alpha=0.7, edgecolor='w')

# Prendre les deux premières dimensions des centroïdes 6D
centroids_uber_6D = kmeans_uber.cluster_centers_
centroids_uber_2D = centroids_uber_6D[:, :2]  # Prendre seulement PC1 et PC2

# Affichage des centroïdes
plt.scatter(centroids_uber_2D[:, 0], centroids_uber_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (4 clusters) après PCA (6D) - Projection sur PC1 & PC2 (Uber) via slicing", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes projetés en 2D (Uber) :")
print(centroids_uber_2D)

# Ajouter les labels de clusters dans le DataFrame Uber (df_pca_uber)
df_pca_uber['Cluster'] = labels_uber

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_uber = df_pca_uber.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Uber :")
print(cluster_summary_uber)

# Visualisation des clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_lyft_2D, x='PC1', y='PC2', hue='Cluster', palette='Set2', s=50, alpha=0.7, edgecolor='w')

# Prendre les deux premières dimensions des centroïdes 6D
centroids_lyft_6D = kmeans_lyft.cluster_centers_
centroids_lyft_2D = centroids_lyft_6D[:, :2]  # Prendre seulement PC1 et PC2

# Affichage des centroïdes
plt.scatter(centroids_lyft_2D[:, 0], centroids_lyft_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (4 clusters) après PCA (6D) - Projection sur PC1 & PC2 (Lyft) via slicing", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes projetés en 2D (Lyft) :")
print(centroids_lyft_2D)

# Ajouter les labels de clusters dans le DataFrame Lyft (df_pca_lyft)
df_pca_lyft['Cluster'] = labels_lyft

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_lyft = df_pca_lyft.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Lyft :")
print(cluster_summary_lyft)

"""5- Evaluation du modèle"""

# Calcul du Silhouette Score pour les clusters de Uber projetés dans l'espace 2D
sil_score_uber = silhouette_score(df_pca_uber_2D[['PC1', 'PC2']], kmeans_uber.labels_)
print(f'Silhouette Score pour Uber: {sil_score_uber:.4f}')

# Calcul du Silhouette Score pour les clusters de Lyft projetés dans l'espace 2D
sil_score_lyft = silhouette_score(df_pca_lyft_2D[['PC1', 'PC2']], kmeans_lyft.labels_)
print(f'Silhouette Score pour Lyft: {sil_score_lyft:.4f}')

# Calcul du Silhouette Score pour les clusters de Uber dans l'espace à 6 dimensions (6 composantes principales)
silhouette_avg_uber = silhouette_score(df_pca_uber_components.drop(columns=['Cluster']), labels_uber)
print(f"Score moyen de silhouette Uber (6D) : {silhouette_avg_uber:.4f}")

# Calcul du Silhouette Score pour les clusters de Lyft dans l'espace à 6 dimensions (6 composantes principales)
silhouette_avg_lyft = silhouette_score(df_pca_lyft_components.drop(columns=['Cluster']), labels_lyft)
print(f"Score moyen de silhouette Lyft (6D) : {silhouette_avg_lyft:.4f}")

# Validation croisée avec Kfold pour évaluation de la stabilité des clusters

# Fonction pour calculer la stabilité des clusters
def kfold_validation(X, n_clusters, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    kmeans_inertia = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        kmeans.fit(X_train)
        kmeans_inertia.append(kmeans.inertia_)  # Inertie sur les données d'entraînement

    # Calculer la moyenne de l'inertie à travers les folds
    mean_inertia = np.mean(kmeans_inertia)
    return mean_inertia

# Validation croisée pour Uber
mean_inertia_uber = kfold_validation(df_pca_uber_components.values, n_clusters=4)
print(f'Moyenne de l\'inertie pour Uber (KFold): {mean_inertia_uber}')

# Validation croisée pour Lyft
mean_inertia_lyft = kfold_validation(df_pca_lyft_components.values, n_clusters=4)
print(f'Moyenne de l\'inertie pour Lyft (KFold): {mean_inertia_lyft}')

# Evaluation de la compacité des clusters

# Inertie pour Uber
inertia_uber = kmeans_uber.inertia_
print(f'Inertie pour Uber: {inertia_uber}')

# Inertie pour Lyft
inertia_lyft = kmeans_lyft.inertia_
print(f'Inertie pour Lyft: {inertia_lyft}')

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Liste des colonnes à conserver
columns_to_keep = [
    "hour", #"day_of_week",
    "hour_sin", "hour_cos",  # Heure (périodique)
    "distance", #"latitude", "longitude",  # Géographie
    "price", #"surge_multiplier",  # Tarification
    "temperature", "humidity" #"windSpeed" # "precipIntensity"  # Météo
]

df_lyft_opti = df_clean[df_clean["cab_type"] == "Lyft"][columns_to_keep].copy()
df_uber_opti = df_clean[df_clean["cab_type"] == "Uber"][columns_to_keep].copy()

# Normalisation des données pour Lyft
scaler_lyft = StandardScaler()
df_pca_lyft_scaled = scaler_lyft.fit_transform(df_lyft_opti)

# Normalisation des données pour Uber
scaler_uber = StandardScaler()
df_pca_uber_scaled = scaler_uber.fit_transform(df_uber_opti)

# Vérification de la forme des matrices normalisées
print("Shape Lyft (normalisé) :", df_pca_lyft_scaled.shape)
print("Shape Uber (normalisé) :", df_pca_uber_scaled.shape)

# PCA pour Lyft
pca_lyft = PCA(n_components=2, svd_solver='auto')
pca_lyft_components = pca_lyft.fit_transform(df_pca_lyft_scaled)

# PCA pour Uber
pca_uber = PCA(n_components=2, svd_solver='auto')
pca_uber_components = pca_uber.fit_transform(df_pca_uber_scaled)

# Création des DataFrames avec les composantes principales pour Lyft et Uber
df_pca_lyft_components = pd.DataFrame(pca_lyft_components, columns=[f'PC{i+1}' for i in range(2)])
df_pca_uber_components = pd.DataFrame(pca_uber_components, columns=[f'PC{i+1}' for i in range(2)])

# Affichage des premières lignes pour vérifier
print("Composantes principales Lyft :")
print(df_pca_lyft_components.head())

print("Composantes principales Uber :")
print(df_pca_uber_components.head())

# Vérification de la variance expliquée par chaque composante principale
print("\nVariance expliquée par chaque composante pour Lyft :")
print(pca_lyft.explained_variance_ratio_)

print("\nVariance expliquée par chaque composante pour Uber :")
print(pca_uber.explained_variance_ratio_)

# Récupérer la variance expliquée par composante et la variance cumulée
explained_variance_ratio_lyft = pca_lyft.explained_variance_ratio_
explained_variance_ratio_uber = pca_uber.explained_variance_ratio_

# Création de la figure
plt.figure(figsize=(10, 6))

# Courbe de variance cumulée pour Lyft
plt.plot(np.arange(1, len(explained_variance_ratio_lyft) + 1), np.cumsum(explained_variance_ratio_lyft),
         marker='o', linestyle='--', color='b', label='Variance cumulée Lyft')

# Barplot de variance par composante pour Lyft
plt.bar(np.arange(1, len(explained_variance_ratio_lyft) + 1), explained_variance_ratio_lyft,
        alpha=0.7, color='#AEC6CF', label='Variance par composante Lyft')

# Courbe de variance cumulée pour Uber
plt.plot(np.arange(1, len(explained_variance_ratio_uber) + 1), np.cumsum(explained_variance_ratio_uber),
         marker='o', linestyle='--', color='g', label='Variance cumulée Uber')

# Barplot de variance par composante pour Uber
plt.bar(np.arange(1, len(explained_variance_ratio_uber) + 1), explained_variance_ratio_uber,
        alpha=0.7, color='#98FB98', label='Variance par composante Uber')

# Axe X avec des valeurs de 1 en 1
plt.xticks(np.arange(1, len(explained_variance_ratio_lyft) + 1, 1))

# Ligne horizontale pour les 90% de variance expliquée
plt.axhline(y=0.9, color='r', linestyle='--', linewidth=1, label='90% de la variance')

# Rallonger l'axe des Y
plt.ylim(0, 1)  # Ajuster la limite de l'axe Y de 0 à 1 pour visualiser la droite à 90% de la variance expliquée

# Labels et titre
plt.xlabel('Composantes principales', fontsize=12, fontweight='bold')
plt.ylabel('Variance expliquée', fontsize=12, fontweight='bold')
plt.title('Courbe des Variances en Fonction des Composantes Principales (Lyft et Uber)', fontsize=14, fontweight='bold')

# Activation du fond gris pour la grille
plt.gca().set_facecolor('#f4f4f4')  # Fond gris

# Ajout d'un quadrillage visible en pointillés gris clair
plt.grid(True, color='gray', linestyle='--', linewidth=0.5)

# Ajout de la légende
plt.legend(frameon=True, facecolor='white', edgecolor='black')

# Affichage du graphique
plt.show()

# Imprimer les composantes principales pour Uber
print("Composantes principales pour Uber:")
print(pca_uber.components_)

# Imprimer les composantes principales pour Lyft
print("Composantes principales pour Lyft:")
print(pca_lyft.components_)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Uber
loadings_uber = pd.DataFrame(pca_uber.components_, columns=columns_to_keep)

# Créer un DataFrame pour les poids (loadings) des composantes principales pour Lyft
loadings_lyft = pd.DataFrame(pca_lyft.components_, columns=columns_to_keep)

# Afficher les DataFrames des poids (loadings) pour Uber et Lyft
print("Loadings pour Uber:")
print(loadings_uber)

print("\nLoadings pour Lyft:")
print(loadings_lyft)

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Fonction pour la méthode du coude
def elbow_method(pca_features, K_range=range(2, 10)):
    inertia = []

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(pca_features)
        inertia.append(kmeans.inertia_)

    # Tracer la courbe du coude
    plt.figure(figsize=(8,5))
    plt.plot(K_range, inertia, marker='o', linestyle='--')
    plt.xlabel('Nombre de clusters')
    plt.ylabel('Inertie (distortion)')
    plt.title('Méthode du coude pour choisir K')
    plt.grid()
    plt.show()

# Appliquer la méthode du coude pour Uber
print("Méthode du coude pour Uber :")
elbow_method(pca_uber_components)  # Appliquer sur les données transformées par PCA pour Uber

# Appliquer la méthode du coude pour Lyft
print("Méthode du coude pour Lyft :")
elbow_method(pca_lyft_components)  # Appliquer sur les données transformées par PCA pour Lyft

# Choisir le nombre optimal de cluster selon la méthode du coude et appliquer Kmeans

# Appliquer KMeans sur les données réduites à 2 dimensions pour Uber
kmeans_uber = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_uber = kmeans_uber.fit_predict(df_pca_uber_components)

# Appliquer KMeans sur les données réduites à 2 dimensions pour Lyft
kmeans_lyft = KMeans(n_clusters=4, random_state=42, n_init=10)
labels_lyft = kmeans_lyft.fit_predict(df_pca_lyft_components)

# Ajouter les labels des clusters
df_pca_uber_components['Cluster'] = labels_uber # Ajouter la colonne "Cluster" avec les labels KMeans
df_pca_lyft_components['Cluster'] = labels_lyft # Ajouter la colonne "Cluster" avec les labels KMeans

# Ajouter les labels des clusters aux DataFrames Uber et Lyft
df_uber_clustered = df_uber_opti.copy()  # Créer une copie du DataFrame Uber
df_uber_clustered['Cluster'] = labels_uber  # Ajouter la colonne "Cluster" avec les labels KMeans

df_lyft_clustered = df_lyft_opti.copy()  # Créer une copie du DataFrame Lyft
df_lyft_clustered['Cluster'] = labels_lyft  # Ajouter la colonne "Cluster" avec les labels KMeans

# Afficher les résultats
print("Clusters Uber:")
print(df_uber_clustered.head())

print("\nClusters Lyft:")
print(df_lyft_clustered.head())

# Visualisation des clusters Uber et clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_uber_components, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_uber_2D = kmeans_uber.cluster_centers_

# Affichage des centroïdes
plt.scatter(centroids_uber_2D[:, 0], centroids_uber_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (4 clusters) après PCA (2D) - Projection sur PC1 & PC2 (Uber)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes Uber :")
print(centroids_uber_2D)

# Ajouter les labels de clusters dans le DataFrame Uber (df_pca_uber)
df_uber_opti['Cluster'] = labels_uber

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_uber = df_uber_opti.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Uber :")
print(cluster_summary_uber)

# Visualisation des clusters Lyft dans un espace 2D avec composante principale n°1 et composante principale n°2

# Appliquer un fond gris pour le style
plt.style.use('bmh')

plt.figure(figsize=(12, 6))
sns.scatterplot(data=df_pca_lyft_components, x='PC1', y='PC2', hue='Cluster', palette='Set2', s=50, alpha=0.7, edgecolor='w')

# Projeter les centroïdes en 2D
centroids_lyft_2D = kmeans_lyft.cluster_centers_

# Affichage des centroïdes
plt.scatter(centroids_lyft_2D[:, 0], centroids_lyft_2D[:, 1], c='black', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (4 clusters) après PCA (2D) - Projection sur PC1 & PC2 (Lyft)", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centroïdes projetés
print("Centroïdes Lyft :")
print(centroids_lyft_2D)

# Ajouter les labels de clusters dans le DataFrame Lyft (df_pca_lyft)
df_lyft_opti['Cluster'] = labels_lyft

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary_lyft = df_lyft_opti.groupby("Cluster")[columns_to_keep].mean()

# Afficher les moyennes
print("Résumé des caractéristiques des clusters pour Lyft :")
print(cluster_summary_lyft)

# Calcul du score de silhouette global dans un espace à 2 dimensions (2 composantes principales)
silhouette_score_uber = silhouette_score(df_pca_uber_components.drop(columns=['Cluster']), labels_uber)
print(f"Score de silhouette uber (2D) : {silhouette_score_uber:.4f}")

silhouette_score_lyft = silhouette_score(df_pca_lyft_components.drop(columns=['Cluster']), labels_lyft)
print(f"Score de silhouette lyft (2D) : {silhouette_score_lyft:.4f}")

# Validation croisée avec Kfold pour évaluation de la stabilité des clusters

# Fonction pour calculer la stabilité des clusters
def kfold_validation(X, n_clusters, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    kmeans_inertia = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        kmeans.fit(X_train)
        kmeans_inertia.append(kmeans.inertia_)  # Inertie sur les données d'entraînement

    # Calculer la moyenne de l'inertie à travers les folds
    mean_inertia = np.mean(kmeans_inertia)
    return mean_inertia

# Validation croisée pour Uber
mean_inertia_uber = kfold_validation(df_pca_uber_components.values, n_clusters=4)
print(f'Moyenne de l\'inertie pour Uber (KFold): {mean_inertia_uber}')

# Validation croisée pour Lyft
mean_inertia_lyft = kfold_validation(df_pca_lyft_components.values, n_clusters=4)
print(f'Moyenne de l\'inertie pour Lyft (KFold): {mean_inertia_lyft}')

# Evaluation de la compacité des clusters

# Inertie pour Uber
inertia_uber = kmeans_uber.inertia_
print(f'Inertie pour Uber: {inertia_uber}')

# Inertie pour Lyft
inertia_lyft = kmeans_lyft.inertia_
print(f'Inertie pour Lyft: {inertia_lyft}')

"""**Actions de suite:**
-Essayer d’autres algorithmes comme DBSCAN (si les clusters ne sont pas de forme sphérique) ou Gaussian Mixture Models (GMM) pour capturer des structures plus complexes

"""