# -*- coding: utf-8 -*-
"""VikenBloc3VTCClustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KECUXXakNfJWPsWWqvEAJNNHDjlC_eDV
"""

# Importation des bibliothèques
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.model_selection import KFold

# Chargement et visualisation du dataset
pd.set_option('display.max_columns', None)
df = pd.read_csv("Bloc3-Ex2-uber.csv")
df.head(5)

# Afficher le nombre de lignes et colonnes du dataframe
df.shape

# Identifier les valeurs manquantes et explorer le type des données du dataframe
df.info()

# Sélectionner les colonnes numériques pertinentes pour le clustering
df_num = df.select_dtypes(include=['int64', 'float64'])
col = ['hour', 'distance', 'price', 'latitude', 'longitude', 'temperature', 'humidity', 'windSpeed']
df_num_col = df_num[col]

# Convertir toutes ces colonnes en float64
df_num_col = df_num_col.astype('float64')

# Vérifier que la conversion a bien eu lieu
print(df_num_col.dtypes)

# Vérifier le résultat
print(df_num_col.head())

# Remplir les valeurs manquantes de la colonne price par la moyenne de la colonne price

# Initialiser l'imputer avec la stratégie "mean"
imputer = SimpleImputer(strategy="mean")

# Appliquer l'imputation sur la colonne 'price' et conserver le résultat dans le DataFrame
df_num_col[['price']] = imputer.fit_transform(df_num_col[['price']])

# Vérifier si les valeurs manquantes ont été remplies
print(df_num_col['price'].isnull().sum())  # Doit afficher 0
df_num_col.info()

# Calculer la skewness de chaque colonne, la trier et l'interpréter
skewness = df_num_col.skew().sort_values(ascending=True)

# Fonction d'interprétation de la skewness
def interpret_skewness(value):
    if abs(value) < 0.5:
        return "Distribution symétrique (proche de la normale)"
    elif 0.5 <= value < 1 or -1 < value <= -0.5:
        return "Légère asymétrie"
    else:
        return "Distribution fortement asymétrique"

# Créer un DataFrame pour afficher proprement
df_skewness = pd.DataFrame({
    'Skewness': skewness,
    'Interprétation': skewness.apply(interpret_skewness)
})

# Afficher le résultat
print(df_skewness)

# Appliquer un style avec un fond gris
plt.style.use("ggplot")

# Définition de la figure
fig = plt.figure(figsize=(18, 8))
fig.patch.set_facecolor('#f5f5f5')  # Fond gris clair

# Boucle pour afficher chaque histogramme
for i, column in enumerate(df_num_col.columns):
    plt.subplot(3, 3, i + 1)
    sns.histplot(df_num_col[column], kde=True, bins=30, color="#3498db")  # Bleu élégant
    plt.title(f"Distribution de {column}", fontsize=11, fontweight='bold', color='#333333')  # Titre en gris foncé
    plt.xlabel("")  # Supprimer le label X pour alléger l'affichage
    plt.ylabel("")

# Ajustement final
plt.tight_layout()
plt.show()

# Vérifier la corrélation entre les colonnes numériques (utile pour PCA)
# Compute the correlation matrix
corr = df_num_col.corr()
print(corr)

sns.set_theme(style="white")

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(corr, dtype=bool))

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(230, 20, as_cmap=True)

#/!\ To get the correct diagonal values(=1) delete the mask from sns.heatmap
# Draw the heatmap with the mask and correct aspect ratio
# mask=mask,
sns.heatmap(corr, cmap=cmap, vmin=-1, vmax=1, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

# Normaisation des colonnes numériques choisies avec StandardScaler()
features = df_num_col[col]
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

pca = PCA(n_components=8, svd_solver='auto')
pca.fit(features_scaled)

reduced_matrix = pca.fit_transform(features_scaled)

reduced_matrix.shape

type(reduced_matrix)

# Variance expliquée par composante
explained_variance_ratio = pca.explained_variance_ratio_

# Création de la figure avec fond blanc (bordure)
plt.figure(figsize=(10, 6))

# Ajout de la courbe de variance cumulée
plt.plot(np.arange(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio),
         marker='o', linestyle='--', color='b', label='Variance cumulée')

# Ajout du barplot pour la variance par composante (barres bleu ciel pastel)
plt.bar(np.arange(1, len(explained_variance_ratio) + 1), explained_variance_ratio,
        alpha=0.7, color='#AEC6CF', label='Variance par composante')  # Bleu ciel pastel

# Axe X avec des valeurs de 1 en 1
plt.xticks(np.arange(1, len(explained_variance_ratio) + 1, 1))

# Ligne horizontale pour les 90% de variance expliquée
plt.axhline(y=0.9, color='r', linestyle='--', linewidth=1, label='90% de la variance')

# Labels et titre
plt.xlabel('Composantes principales', fontsize=12, fontweight='bold')
plt.ylabel('Variance expliquée', fontsize=12, fontweight='bold')
plt.title('Courbe des Variances en Fonction des Composantes Principales', fontsize=14, fontweight='bold')

# Activation du fond gris pour la grille (en arrière-plan)
plt.gca().set_facecolor('#f4f4f4')  # Fond gris derrière la courbe

# Ajout d'un quadrillage visible en pointillés gris clair
plt.grid(True, color='gray', linestyle='--', linewidth=0.5)  # Pointillés gris clair

# Ajout d'une légende stylisée
plt.legend(frameon=True, facecolor='white', edgecolor='black')

# Affichage du graphique
plt.show()

pca.components_

# Créer un DataFrame pour les poids de chaque variable
loadings = pd.DataFrame(pca.components_, columns=col)
print(loadings)

# Application de K-means pour le clustering

# Appliquer un fond gris
plt.style.use('bmh')

# Retenir uniquement les 2 premières composantes principales pour la visualisation
pca_features_2 = reduced_matrix[:, :2]  # Assure-toi que "reduced_matrix" contient bien les données PCA

# Appliquer KMeans avec 3 clusters sur les 2 premières composantes
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(pca_features_2)

# Récupérer les labels des clusters
labels = kmeans.labels_

# Créer un DataFrame avec les 2 premières composantes principales et les labels de clusters
df_pca = pd.DataFrame(pca_features_2, columns=[f'PC{i+1}' for i in range(2)])
df_pca['Cluster'] = labels

# Visualisation des clusters avec les 2 premières composantes principales
plt.figure(figsize=(12, 6))

# Tracer les points des clusters avec des points plus petits (s=60)
sns.scatterplot(x=df_pca['PC1'], y=df_pca['PC2'], hue=df_pca['Cluster'], palette='viridis', s=60, alpha=0.7, edgecolor='w', linewidth=0.5)

# Tracer les centroids sous forme de croix rouge
centroids = kmeans.cluster_centers_  # Récupérer les centres des clusters
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (3 clusters) sur les données PCA", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)

# Ajouter une légende
plt.legend(title="Cluster")
plt.grid(True)

# Afficher le graphique
plt.show()

# Afficher les centres des clusters
print("Centres des clusters :")
print(kmeans.cluster_centers_)




"""# Retenir uniquement les 2 premières composantes principales pour la visualisation
pca_features_6 = reduced_matrix[:, :2]  # Assure-toi que "reduced_matrix" contient bien les données PCA

# Appliquer KMeans avec 3 clusters sur les 2 premières composantes
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(pca_features_6)

# Récupérer les labels des clusters
labels = kmeans.labels_

# Créer un DataFrame avec les 2 premières composantes principales et les labels de clusters
df_pca = pd.DataFrame(pca_features_6, columns=[f'PC{i+1}' for i in range(2)])
df_pca['Cluster'] = labels

# Visualisation des clusters avec les 2 premières composantes principales
plt.figure(figsize=(8, 6))

# Tracer les points des clusters
sns.scatterplot(x=df_pca['PC1'], y=df_pca['PC2'], hue=df_pca['Cluster'], palette='viridis', s=100, alpha=0.7, edgecolor='w', linewidth=0.5)

# Tracer les centroids sous forme de croix rouge
centroids = kmeans.cluster_centers_  # Récupérer les centres des clusters
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroides', linewidth=2)

# Ajouter un titre et labels
plt.title("Clustering K-Means (3 clusters) sur les données PCA", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)

# Ajouter une légende
plt.legend(title="Cluster")
plt.grid(True)

# Afficher le graphique
plt.show()

# Afficher les centres des clusters
print("Centres des clusters :")
print(kmeans.cluster_centers_)"""




"""# Retenir uniquement les 6 premiers composants qui expliquent 90% de la variance
pca_features_6 = reduced_matrix[:, :2]

# Appliquer KMeans avec 3 clusters sur les 6 premières composantes
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(pca_features_6)

# Récupérer les labels des clusters
labels = kmeans.labels_

# Créer un DataFrame avec les composantes principales et les labels de clusters
df_pca = pd.DataFrame(pca_features_6, columns=[f'PC{i+1}' for i in range(2)])
df_pca['Cluster'] = labels

# Visualisation des clusters avec les 2 premières composantes principales
plt.figure(figsize=(8, 6))
sns.scatterplot(x=df_pca['PC1'], y=df_pca['PC2'], hue=df_pca['Cluster'], palette='viridis', s=100, alpha=0.7, edgecolor='w', linewidth=0.5)
plt.title("Clustering K-Means (3 clusters) sur les données PCA", fontsize=14)
plt.xlabel("Composante Principale 1", fontsize=12)
plt.ylabel("Composante Principale 2", fontsize=12)
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Afficher les centres des clusters pour comprendre la position de chaque groupe dans l'espace PCA
print("Centres des clusters :")
print(kmeans.cluster_centers_)"""


"""# Choisir le nombre optimal de clusters (3 à 4 comme suggéré)
# Utilisation de la méthode du coude pour déterminer le K optimal
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(reduced_matrix)
    inertia.append(kmeans.inertia_)

# Affichage de la méthode du coude
plt.plot(range(1, 11), inertia, marker="o")
plt.title("Méthode du coude pour K-Means")
plt.xlabel("Nombre de clusters")
plt.ylabel("Inertie")
plt.show()

# Appliquer K-Means avec k=3 (par exemple)
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(reduced_matrix)

# Ajouter les clusters au DataFrame
df["Cluster"] = clusters

# Visualisation des clusters
plt.scatter(features_pca[:, 0], features_pca[:, 1], c=clusters, cmap="viridis", alpha=0.5)
plt.title("Clusters obtenus après PCA et K-Means")
plt.xlabel("Composante principale 1")
plt.ylabel("Composante principale 2")
plt.colorbar()
plt.show()
"""

# Calculer le Silhouette Score pour évaluer la qualité des clusters sur les 2 premières composantes principales
sil_score = silhouette_score(pca_features_2, labels)

# Afficher le Silhouette Score
print(f"Silhouette Score : {sil_score:.2f}")

# K-Fold Cross-Validation pour évaluer la stabilité des clusters

# Nombre de folds pour la validation croisée
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Liste pour stocker les indices des clusters pour chaque fold
clusters_list = []

# K-Fold Cross Validation
for train_idx, test_idx in kf.split(pca_features_2):  # Utilise les mêmes données que pour PCA
    X_train, X_test = pca_features_2[train_idx], pca_features_2[test_idx]

    # Appliquer KMeans sur l'ensemble d'entraînement
    kmeans = KMeans(n_clusters=3, random_state=42)
    kmeans.fit(X_train)

    # Assigner les clusters aux données de test
    clusters = kmeans.predict(X_test)

    # Ajouter les clusters obtenus pour chaque fold
    clusters_list.append(clusters)

# Calculer la consistance des clusters entre les folds
# Utiliser la distance de Rand ou une autre méthode pour comparer la stabilité des clusters entre les folds

# Comparaison des clusters entre les différents folds
rand_scores = []

for i in range(1, len(clusters_list)):
    score = adjusted_rand_score(clusters_list[0], clusters_list[i])  # Compare le premier fold avec les autres
    rand_scores.append(score)

# Moyenne des scores Rand pour observer la stabilité des clusters
print(f"Stabilité des clusters (score ARI moyen) : {np.mean(rand_scores):.2f}")

# Appliquer KMeans avec différentes initialisations et calculer la variance
n_init_values = 10  # Le nombre d'initialisations
inertia_values = []

for i in range(n_init_values):
    kmeans = KMeans(n_clusters=3, n_init=10, random_state=i)  # Random_state pour des initialisations différentes
    kmeans.fit(pca_features_2)
    inertia_values.append(kmeans.inertia_)

# Afficher la variance de l'inertie des clusters
print(f"Variance de l'inertie sur {n_init_values} initialisations : {np.var(inertia_values):.4f}")

#Courbe de l'inertie en fonction de K
inertia_values = []
for k in range(1, 11):  # Tester différentes valeurs de k
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(pca_features_2)
    inertia_values.append(kmeans.inertia_)

plt.plot(range(1, 11), inertia_values, marker='o', linestyle='--')
plt.title('Évolution de l\'inertie en fonction du nombre de clusters')
plt.xlabel('Nombre de clusters k')
plt.ylabel('Inertie')
plt.title("Courbe de l'inertie en fonction de k")
plt.show()

# Ajouter les labels de clusters dans le DataFrame original (df)
df_num_col['Cluster'] = labels

# Calculer les moyennes des caractéristiques pour chaque cluster
cluster_summary = df_num_col.groupby("Cluster")[col].mean()

# Afficher les moyennes
print(cluster_summary)